{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is the task that was given as a pre requisite for the project 5 of SRIP 2024\n",
    "## Pratham Sharda(pratham.sharda@iitgn.ac.in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with resnet\n",
    "\n",
    "So here first i have divided the dataset for two classes(I have take it for crab and flamingo) and first have run it without crossvalidation due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "traindir = \"D:/SRIP/Binary_task/Training\"\n",
    "testdir = \"D:/SRIP/Binary_task/Validation\"\n",
    "\n",
    "#transformations\n",
    "train_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.ToTensor(),                                \n",
    "                                       torchvision.transforms.Normalize(\n",
    "                                           mean=[0.485, 0.456, 0.406],\n",
    "                                           std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "                                       ])\n",
    "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      torchvision.transforms.Normalize(\n",
    "                                          mean=[0.485, 0.456, 0.406],\n",
    "                                          std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "                                      ])\n",
    "\n",
    "#datasets\n",
    "train_data = datasets.ImageFolder(traindir,transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(testdir,transform=test_transforms)\n",
    "\n",
    "#dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_train_step(model, optimizer, loss_fn):\n",
    "  def train_step(x,y):\n",
    "    #make prediction\n",
    "    yhat = model(x)\n",
    "    #enter train mode\n",
    "    model.train()\n",
    "    #compute loss\n",
    "    loss = loss_fn(yhat,y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    #optimizer.cleargrads()\n",
    "\n",
    "    return loss\n",
    "  return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "#freeze all params\n",
    "for params in model.parameters():\n",
    "  params.requires_grad_ = False\n",
    "\n",
    "#add a new final layer\n",
    "nr_filters = model.fc.in_features  #number of input features of last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#loss\n",
    "loss_fn = BCEWithLogitsLoss() #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters()) \n",
    "\n",
    "#train step\n",
    "train_step = make_train_step(model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "n_epochs = 10\n",
    "early_stopping_tolerance = 3\n",
    "early_stopping_threshold = 0.03\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  epoch_loss = 0\n",
    "  for i ,data in tqdm(enumerate(trainloader), total = len(trainloader)): #iterate ove batches\n",
    "    x_batch , y_batch = data\n",
    "    x_batch = x_batch.to(device) #move to gpu\n",
    "    y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
    "    y_batch = y_batch.to(device) #move to gpu\n",
    "\n",
    "\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "    epoch_loss += loss/len(trainloader)\n",
    "    losses.append(loss)\n",
    "    \n",
    "  epoch_train_losses.append(epoch_loss)\n",
    "  print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
    "\n",
    "  #validation doesnt requires gradient\n",
    "  with torch.no_grad():\n",
    "    cum_loss = 0\n",
    "    for x_batch, y_batch in testloader:\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
    "      y_batch = y_batch.to(device)\n",
    "\n",
    "      #model to eval mode\n",
    "      model.eval()\n",
    "\n",
    "      yhat = model(x_batch)\n",
    "      val_loss = loss_fn(yhat,y_batch)\n",
    "      cum_loss += loss/len(testloader)\n",
    "      val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "    epoch_test_losses.append(cum_loss)\n",
    "    print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))  \n",
    "    \n",
    "    best_loss = min(epoch_test_losses)\n",
    "    \n",
    "    #save best model\n",
    "    if cum_loss <= best_loss:\n",
    "      best_model_wts = model.state_dict()\n",
    "    \n",
    "    #early stopping\n",
    "    early_stopping_counter = 0\n",
    "    if cum_loss > best_loss:\n",
    "      early_stopping_counter +=1\n",
    "\n",
    "    if (early_stopping_counter == early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
    "      print(\"/nTerminating: early stopping\")\n",
    "      break #terminate training\n",
    "    \n",
    "#load best model\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def inference(test_data):\n",
    "  idx = torch.randint(1, len(test_data), (1,))\n",
    "  sample = torch.unsqueeze(test_data[idx][0], dim=0).to(device)\n",
    "\n",
    "  if torch.sigmoid(model(sample)) < 0.5:\n",
    "    print(\"Prediction : Crab\")\n",
    "  else:\n",
    "    print(\"Prediction : Flamingo\")\n",
    "\n",
    "\n",
    "  plt.imshow(test_data[idx][0].permute(1, 2, 0))\n",
    "\n",
    "inference(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with cross validation(3 fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define directories (adjust paths as needed)\n",
    "traindir = \"D:/SRIP/Binary_task/Training\"  # Assuming this now includes both training and validation data\n",
    "\n",
    "# Transformations\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "dataset = datasets.ImageFolder(traindir, transform=transforms)\n",
    "\n",
    "# KFold configuration\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Loss function\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# Cross-validation\n",
    "best_loss_overall = np.inf\n",
    "best_model_wts_overall = None\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"Fold {fold+1}/{kfold.n_splits}\")\n",
    "\n",
    "    # Sampler for splitting data\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Data loaders for training and validation in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "    # Model setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{n_epochs}, Fold {fold+1}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.unsqueeze(1).float()  # Ensure labels are float for BCELoss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader.sampler)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.unsqueeze(1).float()\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(testloader.sampler)\n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}: Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if best_loss < best_loss_overall:\n",
    "        best_loss_overall = best_loss\n",
    "        best_model_wts_overall = copy.deepcopy(best_model_wts)\n",
    "\n",
    "# Load the best model weights from cross-validation\n",
    "model.load_state_dict(best_model_wts_overall)\n",
    "\n",
    "# You can now proceed with testing or inference using this model\n",
    "# For example, here's a simple function to do inference on a single image from the dataset\n",
    "def inference(dataset, model, device):\n",
    "    model.eval()\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    img, _ = dataset[idx]\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension and transfer to device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "\n",
    "    plt.imshow(dataset[idx][0].permute(1, 2, 0))\n",
    "    plt.title(f\"Prediction: {'Flamingo' if prediction >= 0.5 else 'Crab'}\")\n",
    "    plt.show()\n",
    "\n",
    "# Example inference\n",
    "inference(dataset, model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So now i have created the dataset for 5 class and one vs all classification\n",
    "\n",
    "So I have take here for one vs rest classification the example of elephant that is is elephant or is not elephant\n",
    "For 5 class I have converted the dataset into 5 groups where each group is divided based on some particular characteristics(mammals,reptiles,birds,insects and aquatic animals).This has been explicitly mentioned in the notebook below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_conv_layers(model, input_image):\n",
    "#     # Assuming input_image is a torch.Tensor of shape (C, H, W) and normalized\n",
    "#     activation = {}\n",
    "#     def get_activation(name):\n",
    "#         def hook(model, input, output):\n",
    "#             activation[name] = output.detach()\n",
    "#         return hook\n",
    "    \n",
    "#     # Register hooks\n",
    "#     model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "#     model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "#     model.conv3.register_forward_hook(get_activation('conv3'))\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(input_image.unsqueeze(0)) # Add batch dimension\n",
    "    \n",
    "#     # Plotting\n",
    "#     for name, act in activation.items():\n",
    "#         num_feature_maps = act.size(1)\n",
    "#         # Plot feature maps\n",
    "#         # You can use matplotlib to create subplots and plot each feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating one vs rest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source directory where the current dataset is stored\n",
    "source_directory = 'D:/SRIP/archive/animals/animals'\n",
    "\n",
    "# Define the destination directory where the new folder structure will be created\n",
    "destination_directory = 'D:/SRIP/one_vs_rest_dataset'\n",
    "\n",
    "# Define the names of the new subdirectories\n",
    "elephant_dir = os.path.join(destination_directory, 'elephant')\n",
    "other_animals_dir = os.path.join(destination_directory, 'other_animals')\n",
    "\n",
    "# Create the destination directory and subdirectories if they don't already exist\n",
    "os.makedirs(elephant_dir, exist_ok=True)\n",
    "os.makedirs(other_animals_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each folder in the source directory\n",
    "for folder_name in os.listdir(source_directory):\n",
    "    # Define the path to the current folder\n",
    "    current_folder_path = os.path.join(source_directory, folder_name)\n",
    "    \n",
    "    # Check if the current folder is indeed a directory\n",
    "    if os.path.isdir(current_folder_path):\n",
    "        # Determine the destination directory based on whether the folder is 'elephant' or not\n",
    "        if folder_name.lower() == 'elephant':\n",
    "            dest_dir = elephant_dir\n",
    "        else:\n",
    "            dest_dir = other_animals_dir\n",
    "        \n",
    "        # Loop through each file in the current folder\n",
    "        for filename in os.listdir(current_folder_path):\n",
    "            # Define the source and destination file paths\n",
    "            source_file_path = os.path.join(current_folder_path, filename)\n",
    "            destination_file_path = os.path.join(dest_dir, filename)\n",
    "            \n",
    "            # Copy the file from the source to the destination\n",
    "            shutil.copy(source_file_path, destination_file_path)\n",
    "\n",
    "print(\"Dataset reorganization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 fold dataset creation for elephant vs rest animals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory containing the elephant and other_animals folders\n",
    "dataset_directory = 'D:/SRIP/one_vs_rest_dataset'\n",
    "\n",
    "# Define the main categories\n",
    "categories = ['elephant', 'other_animals']\n",
    "\n",
    "# Initialize KFold with 3 splits\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Process each category separately to split them into folds\n",
    "for category in categories:\n",
    "    # Path to the specific category directory\n",
    "    category_path = os.path.join(dataset_directory, category)\n",
    "    \n",
    "    # List all files in the category directory\n",
    "    files = np.array(os.listdir(category_path))\n",
    "    \n",
    "    # Apply KFold splitting\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(files)):\n",
    "        # Paths for train and validation directories for this fold\n",
    "        fold_dir = os.path.join(dataset_directory, f'fold_{fold+1}', category)\n",
    "        \n",
    "        # Create the fold directory if it doesn't exist\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        # Validation files for this fold\n",
    "        val_files = files[val_idx]\n",
    "        \n",
    "        # Copy validation files to the fold directory\n",
    "        for file in val_files:\n",
    "            src_file_path = os.path.join(category_path, file)\n",
    "            dst_file_path = os.path.join(fold_dir, file)\n",
    "            shutil.copy(src_file_path, dst_file_path)\n",
    "\n",
    "print(\"3-Fold dataset split complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Class dataset creation from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define your source and destination folders\n",
    "src_folder = 'D:/SRIP/archive/animals/animals'\n",
    "dest_folder = 'D:/SRIP/5_class_dataset'\n",
    "\n",
    "# Mapping of animals to their classes\n",
    "class_mapping = {\n",
    "    'Mammals': [\"Antelope\", \"Bear\", \"Bison\", \"Cat\", \"Chimpanzee\", \"Cow\", \"Coyote\", \"Deer\", \"Dog\", \"Dolphin\", \"Elephant\", \"Fox\", \"Gorilla\", \"Kangaroo\", \"Koala\", \"Leopard\", \"Lion\", \"Otter\", \"Panda\", \"Porcupine\", \"Raccoon\", \"Reindeer\", \"Rhinoceros\", \"Tiger\", \"Whale\", \"Wolf\", \"Zebra\"],\n",
    "    'Birds': [\"Bat\", \"Eagle\", \"Flamingo\", \"Hummingbird\", \"Owl\", \"Parrot\", \"Pelecaniformes\", \"Penguin\", \"Pigeon\", \"Sparrow\", \"Turkey\", \"Woodpecker\"],\n",
    "    'Aquatic and Amphibious Animals': [\"Crab\", \"Dolphin\", \"Goldfish\", \"Jellyfish\", \"Lobster\", \"Octopus\", \"Oyster\", \"Seahorse\", \"Seal\", \"Shark\", \"Starfish\"],\n",
    "    'Insects and Arthropods': [\"Bee\", \"Beetle\", \"Butterfly\", \"Caterpillar\", \"Cockroach\", \"Dragonfly\", \"Fly\", \"Grasshopper\", \"Ladybugs\", \"Mosquito\", \"Moth\"],\n",
    "    'Reptiles and Others': [\"Lizard\", \"Snake\", \"Turtle\"]  # Assuming 'Crocodile' and 'Tortoise' aren't in your list but could be added if they were.\n",
    "}\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(dest_folder):\n",
    "    os.makedirs(dest_folder)\n",
    "\n",
    "# Create class folders and move files\n",
    "for class_name, animals in class_mapping.items():\n",
    "    class_folder = os.path.join(dest_folder, class_name)\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "    \n",
    "    for animal in animals:\n",
    "        animal_folder = os.path.join(src_folder, animal)\n",
    "        if os.path.exists(animal_folder):\n",
    "            for filename in os.listdir(animal_folder):\n",
    "                src_file = os.path.join(animal_folder, filename)\n",
    "                dest_file = os.path.join(class_folder, filename)\n",
    "                # To avoid overwriting files with the same name from different folders, you could add a check here\n",
    "                shutil.move(src_file, dest_file)\n",
    "\n",
    "print(\"Images have been successfully reorganized into class-based folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Craeting 3 fold dataset from the 5 class dataset so that we can have 3 folds for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory containing the elephant and other_animals folders\n",
    "dataset_directory = 'D:/SRIP/5_class_dataset'\n",
    "\n",
    "# Define the main categories\n",
    "categories = ['Mammals', 'Birds', 'Aquatic and Amphibious Animals', 'Insects and Arthropods', 'Reptiles and Others']\n",
    "\n",
    "# Initialize KFold with 3 splits\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Process each category separately to split them into folds\n",
    "for category in categories:\n",
    "    # Path to the specific category directory\n",
    "    category_path = os.path.join(dataset_directory, category)\n",
    "    \n",
    "    # List all files in the category directory\n",
    "    files = np.array(os.listdir(category_path))\n",
    "    \n",
    "    # Apply KFold splitting\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(files)):\n",
    "        # Paths for train and validation directories for this fold\n",
    "        fold_dir = os.path.join(dataset_directory, f'fold_{fold+1}', category)\n",
    "        \n",
    "        # Create the fold directory if it doesn't exist\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        # Validation files for this fold\n",
    "        val_files = files[val_idx]\n",
    "        \n",
    "        # Copy validation files to the fold directory\n",
    "        for file in val_files:\n",
    "            src_file_path = os.path.join(category_path, file)\n",
    "            dst_file_path = os.path.join(fold_dir, file)\n",
    "            shutil.copy(src_file_path, dst_file_path)\n",
    "\n",
    "print(\"3-Fold dataset split complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=2): # Default binary for one-vs-rest\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(128 * 28 * 28, 512) # Adjust the size according to your input\n",
    "#         self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 28 * 28) # Adjust the size according to your input\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL FOR 5 CLASS \n",
    "\n",
    "Now that we have created the datset for the model.\n",
    "\n",
    "\n",
    "This custom CNN is designed for image classification tasks. It consists of two convolutional layers followed by max pooling layers for feature extraction and spatial downsampling. The convolutional layers use ReLU activation functions. The output from the convolutional layers is flattened and passed through two fully connected layers with ReLU activation and at last having softmax activation function as it is multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=2): # Default binary for one-vs-rest\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(128 * 28 * 28, 512) # Adjust the size according to your input\n",
    "#         self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 28 * 28) # Adjust the size according to your input\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, ConcatDataset\n",
    "# from torchvision import datasets, transforms\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "#         self.fc1 = nn.Linear(64 * 56 * 56, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 5)  # Assuming 5 classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(torch.relu(self.conv1(x)))\n",
    "#         x = self.pool(torch.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 64 * 56 * 56)\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i am dividing each fold into training and test dataset and then fitting on the model and calculating the metrics to check its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_datasets(data_dir, fold):\n",
    "#     image_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "\n",
    "#     train_datasets = []\n",
    "#     test_dataset = None\n",
    "#     for i in range(1, 4):  # For fold 1, fold 2, fold 3\n",
    "#         fold_path = f'{data_dir}/fold_{i}'\n",
    "#         if i == fold:\n",
    "#             test_dataset = datasets.ImageFolder(root=fold_path, transform=image_transform)\n",
    "#         else:\n",
    "#             train_datasets.append(datasets.ImageFolder(root=fold_path, transform=image_transform))\n",
    "\n",
    "#     train_dataset = ConcatDataset(train_datasets)\n",
    "#     return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     23\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 24\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CustomCNN().to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# data_dir = 'D:/folder/SRIP_Task/5_class_dataset'\n",
    "\n",
    "# for fold in range(1, 4):\n",
    "#     print(f\"Training on Fold {fold}\")\n",
    "#     train_dataset, test_dataset = load_datasets(data_dir, fold)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "#     # Training\n",
    "#     model.train()\n",
    "#     for epoch in range(10):  # Number of epochs\n",
    "        \n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "            \n",
    "    \n",
    "#     # Evaluation\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     true_labels = []\n",
    "#     predicted_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#             predicted_labels.extend(predicted.cpu().numpy())\n",
    "#     print(f'Accuracy of the network on fold {fold} test images: {100 * correct / total}%')\n",
    "    \n",
    "#     # Compute confusion matrix\n",
    "#     cm = confusion_matrix(true_labels, predicted_labels)\n",
    "#     sns.heatmap(cm, annot=True, fmt='d')\n",
    "#     plt.title(f'Confusion Matrix for Fold {fold}')\n",
    "#     plt.ylabel('Actual Label')\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = \"D:/folder/SRIP_Task/5_class_dataset/fold_1\"\n",
    "\n",
    "# Define the ratio of images to be used for training (0.8 = 80% training, 0.2 = 20% testing)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "train_dir = os.path.join(folder_path, \"train\")\n",
    "test_dir = os.path.join(folder_path, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List all folders in the main folder\n",
    "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # Get the name of the subfolder (elephant or not elephant)\n",
    "    label = os.path.basename(subfolder)\n",
    "    \n",
    "    # List all images in the subfolder\n",
    "    images = [f for f in os.listdir(subfolder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Calculate the number of images for training\n",
    "    num_train = int(train_ratio * len(images))\n",
    "    \n",
    "    # Split the images into train and test sets\n",
    "    train_images = images[:num_train]\n",
    "    test_images = images[num_train:]\n",
    "    \n",
    "    # Copy train images to the train directory\n",
    "    for image in train_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy test images to the test directory\n",
    "    for image in test_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = \"D:/folder/SRIP_Task/5_class_dataset/fold_2\"\n",
    "\n",
    "# Define the ratio of images to be used for training (0.8 = 80% training, 0.2 = 20% testing)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "train_dir = os.path.join(folder_path, \"train\")\n",
    "test_dir = os.path.join(folder_path, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List all folders in the main folder\n",
    "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # Get the name of the subfolder (elephant or not elephant)\n",
    "    label = os.path.basename(subfolder)\n",
    "    \n",
    "    # List all images in the subfolder\n",
    "    images = [f for f in os.listdir(subfolder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Calculate the number of images for training\n",
    "    num_train = int(train_ratio * len(images))\n",
    "    \n",
    "    # Split the images into train and test sets\n",
    "    train_images = images[:num_train]\n",
    "    test_images = images[num_train:]\n",
    "    \n",
    "    # Copy train images to the train directory\n",
    "    for image in train_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy test images to the test directory\n",
    "    for image in test_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = \"D:/folder/SRIP_Task/5_class_dataset/fold_3\"\n",
    "\n",
    "# Define the ratio of images to be used for training (0.8 = 80% training, 0.2 = 20% testing)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "train_dir = os.path.join(folder_path, \"train\")\n",
    "test_dir = os.path.join(folder_path, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List all folders in the main folder\n",
    "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # Get the name of the subfolder (elephant or not elephant)\n",
    "    label = os.path.basename(subfolder)\n",
    "    \n",
    "    # List all images in the subfolder\n",
    "    images = [f for f in os.listdir(subfolder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Calculate the number of images for training\n",
    "    num_train = int(train_ratio * len(images))\n",
    "    \n",
    "    # Split the images into train and test sets\n",
    "    train_images = images[:num_train]\n",
    "    test_images = images[num_train:]\n",
    "    \n",
    "    # Copy train images to the train directory\n",
    "    for image in train_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy test images to the test directory\n",
    "    for image in test_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 5)  # Assuming 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)  # Flatten the output for the fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(fold_path):\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dir = os.path.join(fold_path, 'train')\n",
    "    test_dir = os.path.join(fold_path, 'test')\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=image_transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=image_transform)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, epochs=15):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        print(f\"Epoch {epoch+1} completed.\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [4], line 10\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parent_dir = 'D:/folder/SRIP_Task/5_class_dataset'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for fold in range(1, 4):  # Assuming there are 3 folds\n",
    "    print(f\"Training on Fold {fold}\")\n",
    "    fold_path = f'{parent_dir}/fold_{fold}'\n",
    "    train_dataset, test_dataset = load_datasets(fold_path)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, epochs=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one vs rest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = \"D:/folder/SRIP_Task/one_vs_rest_dataset/fold_1\"\n",
    "\n",
    "# Define the ratio of images to be used for training (0.8 = 80% training, 0.2 = 20% testing)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "train_dir = os.path.join(folder_path, \"train\")\n",
    "test_dir = os.path.join(folder_path, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List all folders in the main folder\n",
    "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # Get the name of the subfolder (elephant or not elephant)\n",
    "    label = os.path.basename(subfolder)\n",
    "    \n",
    "    # List all images in the subfolder\n",
    "    images = [f for f in os.listdir(subfolder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Calculate the number of images for training\n",
    "    num_train = int(train_ratio * len(images))\n",
    "    \n",
    "    # Split the images into train and test sets\n",
    "    train_images = images[:num_train]\n",
    "    test_images = images[num_train:]\n",
    "    \n",
    "    # Copy train images to the train directory\n",
    "    for image in train_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy test images to the test directory\n",
    "    for image in test_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = \"D:/folder/SRIP_Task/one_vs_rest_dataset/fold_2\"\n",
    "\n",
    "# Define the ratio of images to be used for training (0.8 = 80% training, 0.2 = 20% testing)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "train_dir = os.path.join(folder_path, \"train\")\n",
    "test_dir = os.path.join(folder_path, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List all folders in the main folder\n",
    "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # Get the name of the subfolder (elephant or not elephant)\n",
    "    label = os.path.basename(subfolder)\n",
    "    \n",
    "    # List all images in the subfolder\n",
    "    images = [f for f in os.listdir(subfolder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Calculate the number of images for training\n",
    "    num_train = int(train_ratio * len(images))\n",
    "    \n",
    "    # Split the images into train and test sets\n",
    "    train_images = images[:num_train]\n",
    "    test_images = images[num_train:]\n",
    "    \n",
    "    # Copy train images to the train directory\n",
    "    for image in train_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy test images to the test directory\n",
    "    for image in test_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to the folder containing the images\n",
    "folder_path = \"D:/folder/SRIP_Task/one_vs_rest_dataset/fold_3\"\n",
    "\n",
    "# Define the ratio of images to be used for training (0.8 = 80% training, 0.2 = 20% testing)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test directories if they don't exist\n",
    "train_dir = os.path.join(folder_path, \"train\")\n",
    "test_dir = os.path.join(folder_path, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# List all folders in the main folder\n",
    "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    # Get the name of the subfolder (elephant or not elephant)\n",
    "    label = os.path.basename(subfolder)\n",
    "    \n",
    "    # List all images in the subfolder\n",
    "    images = [f for f in os.listdir(subfolder) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Calculate the number of images for training\n",
    "    num_train = int(train_ratio * len(images))\n",
    "    \n",
    "    # Split the images into train and test sets\n",
    "    train_images = images[:num_train]\n",
    "    test_images = images[num_train:]\n",
    "    \n",
    "    # Copy train images to the train directory\n",
    "    for image in train_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy test images to the test directory\n",
    "    for image in test_images:\n",
    "        src = os.path.join(subfolder, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "print(\"Dataset split completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Assuming 3-channel input images\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # Adjust size according to your image size\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)  # Adjust flattening size according to your image size\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def get_data_loader(root_dir, fold, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Adjust the path to where your data is stored\n",
    "    train_data = ImageFolder(root=os.path.join(root_dir, f'fold_{fold}', 'train'), transform=transform)\n",
    "    test_data = ImageFolder(root=os.path.join(root_dir, f'fold_{fold}', 'test'), transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the test set: {accuracy}%')\n",
    "\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Not Elephant', 'Elephant']))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(cm)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1\n",
      "Epoch 1, Loss: 0.6961296200752258\n",
      "Epoch 1, Loss: 0.327700138092041\n",
      "Epoch 1, Loss: 0.04799414426088333\n",
      "Epoch 1, Loss: 0.0015845266170799732\n",
      "Epoch 1, Loss: 8.861989044817165e-05\n",
      "Epoch 1, Loss: 0.5008435845375061\n",
      "Epoch 1, Loss: 2.045147539320169e-06\n",
      "Epoch 1, Loss: 1.7806787582230754e-06\n",
      "Epoch 1, Loss: 3.1701736133982195e-06\n",
      "Epoch 1, Loss: 3.0397823138628155e-06\n",
      "Epoch 1, Loss: 2.2277015432337066e-06\n",
      "Epoch 1, Loss: 7.081425792421214e-06\n",
      "Epoch 1, Loss: 2.5108149657171452e-06\n",
      "Epoch 1, Loss: 2.3208322090795264e-06\n",
      "Epoch 1, Loss: 0.4275161027908325\n",
      "Epoch 1, Loss: 0.3947215676307678\n",
      "Epoch 1, Loss: 0.4502102732658386\n",
      "Epoch 1, Loss: 0.000518253946211189\n",
      "Epoch 1, Loss: 0.0022971753496676683\n",
      "Epoch 1, Loss: 0.005846443120390177\n",
      "Epoch 1, Loss: 0.013880755752325058\n",
      "Epoch 1, Loss: 0.024269351735711098\n",
      "Epoch 1, Loss: 0.03464173525571823\n",
      "Epoch 1, Loss: 0.035341206938028336\n",
      "Epoch 1, Loss: 0.13371872901916504\n",
      "Epoch 1, Loss: 0.1361151784658432\n",
      "Epoch 1, Loss: 0.03951061889529228\n",
      "Epoch 1, Loss: 0.13735561072826385\n",
      "Epoch 1, Loss: 0.02694656141102314\n",
      "Epoch 1, Loss: 0.020405137911438942\n",
      "Epoch 1, Loss: 0.158247709274292\n",
      "Epoch 1, Loss: 0.13806548714637756\n",
      "Epoch 1, Loss: 0.14678630232810974\n",
      "Epoch 1, Loss: 0.01008399948477745\n",
      "Epoch 1, Loss: 0.011024624109268188\n",
      "Epoch 1, Loss: 0.14756275713443756\n",
      "Epoch 1, Loss: 0.20121057331562042\n",
      "Epoch 1, Loss: 0.013444376178085804\n",
      "Epoch 1, Loss: 0.013698527589440346\n",
      "Epoch 1, Loss: 0.0155177628621459\n",
      "Epoch 1, Loss: 0.011965255253016949\n",
      "Epoch 1, Loss: 0.011977443471550941\n",
      "Epoch 1, Loss: 0.011203862726688385\n",
      "Epoch 1, Loss: 0.008381057530641556\n",
      "Epoch 1, Loss: 0.18422763049602509\n",
      "Epoch 1, Loss: 0.1009123751365981\n",
      "Epoch 2, Loss: 0.1955248862504959\n",
      "Epoch 2, Loss: 0.007230136077851057\n",
      "Epoch 2, Loss: 0.006742045283317566\n",
      "Epoch 2, Loss: 0.009445840492844582\n",
      "Epoch 2, Loss: 0.009724581614136696\n",
      "Epoch 2, Loss: 0.17915396392345428\n",
      "Epoch 2, Loss: 0.1505492925643921\n",
      "Epoch 2, Loss: 0.15151014924049377\n",
      "Epoch 2, Loss: 0.01618070714175701\n",
      "Epoch 2, Loss: 0.13827231526374817\n",
      "Epoch 2, Loss: 0.021944968029856682\n",
      "Epoch 2, Loss: 0.02711046300828457\n",
      "Epoch 2, Loss: 0.025360746309161186\n",
      "Epoch 2, Loss: 0.022876450791954994\n",
      "Epoch 2, Loss: 0.020418353378772736\n",
      "Epoch 2, Loss: 0.15591029822826385\n",
      "Epoch 2, Loss: 0.009068608283996582\n",
      "Epoch 2, Loss: 0.007900618948042393\n",
      "Epoch 2, Loss: 0.0065396991558372974\n",
      "Epoch 2, Loss: 0.004959757439792156\n",
      "Epoch 2, Loss: 0.184073343873024\n",
      "Epoch 2, Loss: 0.003353926818817854\n",
      "Epoch 2, Loss: 0.00322701851837337\n",
      "Epoch 2, Loss: 0.1917112022638321\n",
      "Epoch 2, Loss: 0.00354198575951159\n",
      "Epoch 2, Loss: 0.16695116460323334\n",
      "Epoch 2, Loss: 0.1681738644838333\n",
      "Epoch 2, Loss: 0.15135926008224487\n",
      "Epoch 2, Loss: 0.012754428200423717\n",
      "Epoch 2, Loss: 0.018002977594733238\n",
      "Epoch 2, Loss: 0.024215908721089363\n",
      "Epoch 2, Loss: 0.02619333192706108\n",
      "Epoch 2, Loss: 0.026070604100823402\n",
      "Epoch 2, Loss: 0.1393418312072754\n",
      "Epoch 2, Loss: 0.018980473279953003\n",
      "Epoch 2, Loss: 0.017464157193899155\n",
      "Epoch 2, Loss: 0.012315887957811356\n",
      "Epoch 2, Loss: 0.009867658838629723\n",
      "Epoch 2, Loss: 0.1758035570383072\n",
      "Epoch 2, Loss: 0.16822755336761475\n",
      "Epoch 2, Loss: 0.006948886439204216\n",
      "Epoch 2, Loss: 0.007026171311736107\n",
      "Epoch 2, Loss: 0.006343907210975885\n",
      "Epoch 2, Loss: 0.006809727288782597\n",
      "Epoch 2, Loss: 0.34632647037506104\n",
      "Epoch 2, Loss: 0.06803353733072678\n",
      "Epoch 3, Loss: 0.007282701320946217\n",
      "Epoch 3, Loss: 0.010139642283320427\n",
      "Epoch 3, Loss: 0.0125404829159379\n",
      "Epoch 3, Loss: 0.010813770815730095\n",
      "Epoch 3, Loss: 0.01162951160222292\n",
      "Epoch 3, Loss: 0.14575742185115814\n",
      "Epoch 3, Loss: 0.14080272614955902\n",
      "Epoch 3, Loss: 0.01274938602000475\n",
      "Epoch 3, Loss: 0.013614569790661335\n",
      "Epoch 3, Loss: 0.013902878388762474\n",
      "Epoch 3, Loss: 0.01287018321454525\n",
      "Epoch 3, Loss: 0.17954231798648834\n",
      "Epoch 3, Loss: 0.14560098946094513\n",
      "Epoch 3, Loss: 0.012404950335621834\n",
      "Epoch 3, Loss: 0.012733611278235912\n",
      "Epoch 3, Loss: 0.1371145248413086\n",
      "Epoch 3, Loss: 0.014812525361776352\n",
      "Epoch 3, Loss: 0.014812944456934929\n",
      "Epoch 3, Loss: 0.1364860087633133\n",
      "Epoch 3, Loss: 0.013320106081664562\n",
      "Epoch 3, Loss: 0.14889073371887207\n",
      "Epoch 3, Loss: 0.15574567019939423\n",
      "Epoch 3, Loss: 0.13658426702022552\n",
      "Epoch 3, Loss: 0.017857534810900688\n",
      "Epoch 3, Loss: 0.020898323506116867\n",
      "Epoch 3, Loss: 0.14827609062194824\n",
      "Epoch 3, Loss: 0.022266961634159088\n",
      "Epoch 3, Loss: 0.02149021066725254\n",
      "Epoch 3, Loss: 0.01871049962937832\n",
      "Epoch 3, Loss: 0.014133552089333534\n",
      "Epoch 3, Loss: 0.28328514099121094\n",
      "Epoch 3, Loss: 0.1445266157388687\n",
      "Epoch 3, Loss: 0.013514316640794277\n",
      "Epoch 3, Loss: 0.014080045744776726\n",
      "Epoch 3, Loss: 0.014379890635609627\n",
      "Epoch 3, Loss: 0.14668165147304535\n",
      "Epoch 3, Loss: 0.15372811257839203\n",
      "Epoch 3, Loss: 0.015029666014015675\n",
      "Epoch 3, Loss: 0.014832661487162113\n",
      "Epoch 3, Loss: 0.01586912013590336\n",
      "Epoch 3, Loss: 0.013275042176246643\n",
      "Epoch 3, Loss: 0.15657591819763184\n",
      "Epoch 3, Loss: 0.0108481515198946\n",
      "Epoch 3, Loss: 0.008666911162436008\n",
      "Epoch 3, Loss: 0.008788328617811203\n",
      "Epoch 3, Loss: 0.06173037044290039\n",
      "Epoch 4, Loss: 0.005563441663980484\n",
      "Epoch 4, Loss: 0.004738952498883009\n",
      "Epoch 4, Loss: 0.18881598114967346\n",
      "Epoch 4, Loss: 0.003697057953104377\n",
      "Epoch 4, Loss: 0.3549267649650574\n",
      "Epoch 4, Loss: 0.006071778014302254\n",
      "Epoch 4, Loss: 0.009694723412394524\n",
      "Epoch 4, Loss: 0.15889360010623932\n",
      "Epoch 4, Loss: 0.017729301005601883\n",
      "Epoch 4, Loss: 0.023709945380687714\n",
      "Epoch 4, Loss: 0.024984240531921387\n",
      "Epoch 4, Loss: 0.023791566491127014\n",
      "Epoch 4, Loss: 0.01987585425376892\n",
      "Epoch 4, Loss: 0.14108459651470184\n",
      "Epoch 4, Loss: 0.2632910907268524\n",
      "Epoch 4, Loss: 0.015486798249185085\n",
      "Epoch 4, Loss: 0.016340479254722595\n",
      "Epoch 4, Loss: 0.01620042510330677\n",
      "Epoch 4, Loss: 0.2589205801486969\n",
      "Epoch 4, Loss: 0.014591730199754238\n",
      "Epoch 4, Loss: 0.015583123080432415\n",
      "Epoch 4, Loss: 0.1271136850118637\n",
      "Epoch 4, Loss: 0.014137367717921734\n",
      "Epoch 4, Loss: 0.132065549492836\n",
      "Epoch 4, Loss: 0.016640296205878258\n",
      "Epoch 4, Loss: 0.14961864054203033\n",
      "Epoch 4, Loss: 0.01433198805898428\n",
      "Epoch 4, Loss: 0.14703749120235443\n",
      "Epoch 4, Loss: 0.01694299839437008\n",
      "Epoch 4, Loss: 0.014056803658604622\n",
      "Epoch 4, Loss: 0.1351846158504486\n",
      "Epoch 4, Loss: 0.013690358027815819\n",
      "Epoch 4, Loss: 0.012286691926419735\n",
      "Epoch 4, Loss: 0.011610816232860088\n",
      "Epoch 4, Loss: 0.008487957529723644\n",
      "Epoch 4, Loss: 0.005643324926495552\n",
      "Epoch 4, Loss: 0.003968939185142517\n",
      "Epoch 4, Loss: 0.0027874773368239403\n",
      "Epoch 4, Loss: 0.17998726665973663\n",
      "Epoch 4, Loss: 0.00201017945073545\n",
      "Epoch 4, Loss: 0.002427870174869895\n",
      "Epoch 4, Loss: 0.19635865092277527\n",
      "Epoch 4, Loss: 0.0030145945493131876\n",
      "Epoch 4, Loss: 0.003434840589761734\n",
      "Epoch 4, Loss: 0.004315436352044344\n",
      "Epoch 4, Loss: 0.06224768601564897\n",
      "Epoch 5, Loss: 0.004771163687109947\n",
      "Epoch 5, Loss: 0.004966686014086008\n",
      "Epoch 5, Loss: 0.004172098822891712\n",
      "Epoch 5, Loss: 0.1777133196592331\n",
      "Epoch 5, Loss: 0.006020560394972563\n",
      "Epoch 5, Loss: 0.008040575310587883\n",
      "Epoch 5, Loss: 0.005735356826335192\n",
      "Epoch 5, Loss: 0.006691329181194305\n",
      "Epoch 5, Loss: 0.005799084436148405\n",
      "Epoch 5, Loss: 0.006268051918596029\n",
      "Epoch 5, Loss: 0.004600449465215206\n",
      "Epoch 5, Loss: 0.004053704906255007\n",
      "Epoch 5, Loss: 0.0031285046134144068\n",
      "Epoch 5, Loss: 0.002799085807055235\n",
      "Epoch 5, Loss: 0.0019567993003875017\n",
      "Epoch 5, Loss: 0.0016651592450216413\n",
      "Epoch 5, Loss: 0.001338573289103806\n",
      "Epoch 5, Loss: 0.0009907814674079418\n",
      "Epoch 5, Loss: 0.3974948525428772\n",
      "Epoch 5, Loss: 0.0013268906623125076\n",
      "Epoch 5, Loss: 0.002079269615933299\n",
      "Epoch 5, Loss: 0.15636996924877167\n",
      "Epoch 5, Loss: 0.006166818086057901\n",
      "Epoch 5, Loss: 0.009623820893466473\n",
      "Epoch 5, Loss: 0.14260654151439667\n",
      "Epoch 5, Loss: 0.2541380524635315\n",
      "Epoch 5, Loss: 0.031867895275354385\n",
      "Epoch 5, Loss: 0.1466618776321411\n",
      "Epoch 5, Loss: 0.052969466894865036\n",
      "Epoch 5, Loss: 0.052303846925497055\n",
      "Epoch 5, Loss: 0.11788143217563629\n",
      "Epoch 5, Loss: 0.03172764927148819\n",
      "Epoch 5, Loss: 0.02294979989528656\n",
      "Epoch 5, Loss: 0.016811782494187355\n",
      "Epoch 5, Loss: 0.13719864189624786\n",
      "Epoch 5, Loss: 0.16940076649188995\n",
      "Epoch 5, Loss: 0.16206781566143036\n",
      "Epoch 5, Loss: 0.1458878517150879\n",
      "Epoch 5, Loss: 0.007724982686340809\n",
      "Epoch 5, Loss: 0.13946041464805603\n",
      "Epoch 5, Loss: 0.013543324545025826\n",
      "Epoch 5, Loss: 0.013746649958193302\n",
      "Epoch 5, Loss: 0.15299761295318604\n",
      "Epoch 5, Loss: 0.01757187210023403\n",
      "Epoch 5, Loss: 0.1338086873292923\n",
      "Epoch 5, Loss: 0.061935552664928964\n",
      "Epoch 6, Loss: 0.020916476845741272\n",
      "Epoch 6, Loss: 0.02197524718940258\n",
      "Epoch 6, Loss: 0.01832893304526806\n",
      "Epoch 6, Loss: 0.015485074371099472\n",
      "Epoch 6, Loss: 0.1321537047624588\n",
      "Epoch 6, Loss: 0.14471444487571716\n",
      "Epoch 6, Loss: 0.008268395438790321\n",
      "Epoch 6, Loss: 0.2523469626903534\n",
      "Epoch 6, Loss: 0.1319420337677002\n",
      "Epoch 6, Loss: 0.01704932563006878\n",
      "Epoch 6, Loss: 0.020799005404114723\n",
      "Epoch 6, Loss: 0.019643904641270638\n",
      "Epoch 6, Loss: 0.1417400985956192\n",
      "Epoch 6, Loss: 0.01919206790626049\n",
      "Epoch 6, Loss: 0.01694890484213829\n",
      "Epoch 6, Loss: 0.12066121399402618\n",
      "Epoch 6, Loss: 0.014179140329360962\n",
      "Epoch 6, Loss: 0.12824825942516327\n",
      "Epoch 6, Loss: 0.010566065087914467\n",
      "Epoch 6, Loss: 0.009887940250337124\n",
      "Epoch 6, Loss: 0.136906236410141\n",
      "Epoch 6, Loss: 0.006471790373325348\n",
      "Epoch 6, Loss: 0.00857395026832819\n",
      "Epoch 6, Loss: 0.00422257836908102\n",
      "Epoch 6, Loss: 0.2284027636051178\n",
      "Epoch 6, Loss: 0.007038992363959551\n",
      "Epoch 6, Loss: 0.007312832400202751\n",
      "Epoch 6, Loss: 0.009523067623376846\n",
      "Epoch 6, Loss: 0.010102818720042706\n",
      "Epoch 6, Loss: 0.007717061787843704\n",
      "Epoch 6, Loss: 0.007636071648448706\n",
      "Epoch 6, Loss: 0.13920292258262634\n",
      "Epoch 6, Loss: 0.006159165874123573\n",
      "Epoch 6, Loss: 0.16257625818252563\n",
      "Epoch 6, Loss: 0.11480562388896942\n",
      "Epoch 6, Loss: 0.11060961335897446\n",
      "Epoch 6, Loss: 0.016085200011730194\n",
      "Epoch 6, Loss: 0.019944151863455772\n",
      "Epoch 6, Loss: 0.019118845462799072\n",
      "Epoch 6, Loss: 0.02074861153960228\n",
      "Epoch 6, Loss: 0.009349295869469643\n",
      "Epoch 6, Loss: 0.14098387956619263\n",
      "Epoch 6, Loss: 0.009097444824874401\n",
      "Epoch 6, Loss: 0.009571531787514687\n",
      "Epoch 6, Loss: 0.1678573489189148\n",
      "Epoch 6, Loss: 0.058779227919876574\n",
      "Epoch 7, Loss: 0.11559340357780457\n",
      "Epoch 7, Loss: 0.009734350256621838\n",
      "Epoch 7, Loss: 0.014331774786114693\n",
      "Epoch 7, Loss: 0.01196109689772129\n",
      "Epoch 7, Loss: 0.15003451704978943\n",
      "Epoch 7, Loss: 0.013873088173568249\n",
      "Epoch 7, Loss: 0.014633359387516975\n",
      "Epoch 7, Loss: 0.010926195420324802\n",
      "Epoch 7, Loss: 0.015471572056412697\n",
      "Epoch 7, Loss: 0.10208810865879059\n",
      "Epoch 7, Loss: 0.00687055429443717\n",
      "Epoch 7, Loss: 0.25930386781692505\n",
      "Epoch 7, Loss: 0.009806454181671143\n",
      "Epoch 7, Loss: 0.009195012040436268\n",
      "Epoch 7, Loss: 0.013771557249128819\n",
      "Epoch 7, Loss: 0.05224166810512543\n",
      "Epoch 7, Loss: 0.10047372430562973\n",
      "Epoch 7, Loss: 0.11235466599464417\n",
      "Epoch 7, Loss: 0.0133928582072258\n",
      "Epoch 7, Loss: 0.020083460956811905\n",
      "Epoch 7, Loss: 0.08477246016263962\n",
      "Epoch 7, Loss: 0.017488520592451096\n",
      "Epoch 7, Loss: 0.01615719497203827\n",
      "Epoch 7, Loss: 0.01221396867185831\n",
      "Epoch 7, Loss: 0.20692768692970276\n",
      "Epoch 7, Loss: 0.004695893730968237\n",
      "Epoch 7, Loss: 0.1293136477470398\n",
      "Epoch 7, Loss: 0.005982638336718082\n",
      "Epoch 7, Loss: 0.004106162581592798\n",
      "Epoch 7, Loss: 0.10521557927131653\n",
      "Epoch 7, Loss: 0.010534406639635563\n",
      "Epoch 7, Loss: 0.005807004868984222\n",
      "Epoch 7, Loss: 0.1258866786956787\n",
      "Epoch 7, Loss: 0.01328796986490488\n",
      "Epoch 7, Loss: 0.014347868971526623\n",
      "Epoch 7, Loss: 0.014318373054265976\n",
      "Epoch 7, Loss: 0.010634471662342548\n",
      "Epoch 7, Loss: 0.009811058640480042\n",
      "Epoch 7, Loss: 0.004019451793283224\n",
      "Epoch 7, Loss: 0.16970090568065643\n",
      "Epoch 7, Loss: 0.003676517168059945\n",
      "Epoch 7, Loss: 0.35676753520965576\n",
      "Epoch 7, Loss: 0.005699705332517624\n",
      "Epoch 7, Loss: 0.013836701400578022\n",
      "Epoch 7, Loss: 0.017898432910442352\n",
      "Epoch 7, Loss: 0.053760936095689736\n",
      "Epoch 8, Loss: 0.10794103890657425\n",
      "Epoch 8, Loss: 0.03889480605721474\n",
      "Epoch 8, Loss: 0.023682700470089912\n",
      "Epoch 8, Loss: 0.013285746797919273\n",
      "Epoch 8, Loss: 0.010738573968410492\n",
      "Epoch 8, Loss: 0.23703326284885406\n",
      "Epoch 8, Loss: 0.008097711019217968\n",
      "Epoch 8, Loss: 0.004378371872007847\n",
      "Epoch 8, Loss: 0.07667794078588486\n",
      "Epoch 8, Loss: 0.011391321197152138\n",
      "Epoch 8, Loss: 0.008721742779016495\n",
      "Epoch 8, Loss: 0.07888136059045792\n",
      "Epoch 8, Loss: 0.007950989529490471\n",
      "Epoch 8, Loss: 0.10119535773992538\n",
      "Epoch 8, Loss: 0.018363773822784424\n",
      "Epoch 8, Loss: 0.015311072580516338\n",
      "Epoch 8, Loss: 0.18129374086856842\n",
      "Epoch 8, Loss: 0.019750935956835747\n",
      "Epoch 8, Loss: 0.022847693413496017\n",
      "Epoch 8, Loss: 0.022904306650161743\n",
      "Epoch 8, Loss: 0.017653491348028183\n",
      "Epoch 8, Loss: 0.006820814684033394\n",
      "Epoch 8, Loss: 0.0046348064206540585\n",
      "Epoch 8, Loss: 0.0017549324547871947\n",
      "Epoch 8, Loss: 0.22984783351421356\n",
      "Epoch 8, Loss: 0.002003318164497614\n",
      "Epoch 8, Loss: 0.0020654024556279182\n",
      "Epoch 8, Loss: 0.3155340552330017\n",
      "Epoch 8, Loss: 0.0032711580861359835\n",
      "Epoch 8, Loss: 0.09510893374681473\n",
      "Epoch 8, Loss: 0.10213515907526016\n",
      "Epoch 8, Loss: 0.02584272436797619\n",
      "Epoch 8, Loss: 0.03931800276041031\n",
      "Epoch 8, Loss: 0.10591015219688416\n",
      "Epoch 8, Loss: 0.04439922422170639\n",
      "Epoch 8, Loss: 0.025277968496084213\n",
      "Epoch 8, Loss: 0.016174806281924248\n",
      "Epoch 8, Loss: 0.005459910724312067\n",
      "Epoch 8, Loss: 0.0022441477049142122\n",
      "Epoch 8, Loss: 0.16846290230751038\n",
      "Epoch 8, Loss: 0.0018361324910074472\n",
      "Epoch 8, Loss: 0.0010104228276759386\n",
      "Epoch 8, Loss: 0.13473373651504517\n",
      "Epoch 8, Loss: 0.0008801391813904047\n",
      "Epoch 8, Loss: 0.0006504495395347476\n",
      "Epoch 8, Loss: 0.052497179392311306\n",
      "Epoch 9, Loss: 0.0016102471854537725\n",
      "Epoch 9, Loss: 0.19066311419010162\n",
      "Epoch 9, Loss: 0.002372452989220619\n",
      "Epoch 9, Loss: 0.004112983588129282\n",
      "Epoch 9, Loss: 0.0027581974864006042\n",
      "Epoch 9, Loss: 0.007876715622842312\n",
      "Epoch 9, Loss: 0.0066344160586595535\n",
      "Epoch 9, Loss: 0.0049620275385677814\n",
      "Epoch 9, Loss: 0.174829363822937\n",
      "Epoch 9, Loss: 0.0068283770233392715\n",
      "Epoch 9, Loss: 0.008439082652330399\n",
      "Epoch 9, Loss: 0.007649253588169813\n",
      "Epoch 9, Loss: 0.04354579374194145\n",
      "Epoch 9, Loss: 0.011908459477126598\n",
      "Epoch 9, Loss: 0.009034671820700169\n",
      "Epoch 9, Loss: 0.005940013565123081\n",
      "Epoch 9, Loss: 0.08695962280035019\n",
      "Epoch 9, Loss: 0.005945808719843626\n",
      "Epoch 9, Loss: 0.1022711768746376\n",
      "Epoch 9, Loss: 0.005376157816499472\n",
      "Epoch 9, Loss: 0.003773922799155116\n",
      "Epoch 9, Loss: 0.004999267403036356\n",
      "Epoch 9, Loss: 0.004184290766716003\n",
      "Epoch 9, Loss: 0.09073825925588608\n",
      "Epoch 9, Loss: 0.0020756262820214033\n",
      "Epoch 9, Loss: 0.12301068753004074\n",
      "Epoch 9, Loss: 0.07782889157533646\n",
      "Epoch 9, Loss: 0.01281117182224989\n",
      "Epoch 9, Loss: 0.018492398783564568\n",
      "Epoch 9, Loss: 0.09375926852226257\n",
      "Epoch 9, Loss: 0.02105589210987091\n",
      "Epoch 9, Loss: 0.02573385089635849\n",
      "Epoch 9, Loss: 0.026170894503593445\n",
      "Epoch 9, Loss: 0.2132386416196823\n",
      "Epoch 9, Loss: 0.019562723115086555\n",
      "Epoch 9, Loss: 0.10082177817821503\n",
      "Epoch 9, Loss: 0.005915348418056965\n",
      "Epoch 9, Loss: 0.011355801485478878\n",
      "Epoch 9, Loss: 0.037956077605485916\n",
      "Epoch 9, Loss: 0.010654404759407043\n",
      "Epoch 9, Loss: 0.005491409916430712\n",
      "Epoch 9, Loss: 0.08221110701560974\n",
      "Epoch 9, Loss: 0.0024127927608788013\n",
      "Epoch 9, Loss: 0.06757038831710815\n",
      "Epoch 9, Loss: 0.005741446278989315\n",
      "Epoch 9, Loss: 0.0390507617396199\n",
      "Epoch 10, Loss: 0.004064120352268219\n",
      "Epoch 10, Loss: 0.004203234799206257\n",
      "Epoch 10, Loss: 0.0039850384928286076\n",
      "Epoch 10, Loss: 0.002849944867193699\n",
      "Epoch 10, Loss: 0.0019521459471434355\n",
      "Epoch 10, Loss: 0.001906619523651898\n",
      "Epoch 10, Loss: 0.12166580557823181\n",
      "Epoch 10, Loss: 0.16464194655418396\n",
      "Epoch 10, Loss: 0.0031307495664805174\n",
      "Epoch 10, Loss: 0.13007549941539764\n",
      "Epoch 10, Loss: 0.019797025248408318\n",
      "Epoch 10, Loss: 0.04062589630484581\n",
      "Epoch 10, Loss: 0.04267670214176178\n",
      "Epoch 10, Loss: 0.01906607300043106\n",
      "Epoch 10, Loss: 0.025757040828466415\n",
      "Epoch 10, Loss: 0.009691230952739716\n",
      "Epoch 10, Loss: 0.00930624920874834\n",
      "Epoch 10, Loss: 0.0018038010457530618\n",
      "Epoch 10, Loss: 0.054965704679489136\n",
      "Epoch 10, Loss: 0.09673257172107697\n",
      "Epoch 10, Loss: 0.0006478074355982244\n",
      "Epoch 10, Loss: 0.0007005040533840656\n",
      "Epoch 10, Loss: 0.06639587134122849\n",
      "Epoch 10, Loss: 0.09255366772413254\n",
      "Epoch 10, Loss: 0.04958207160234451\n",
      "Epoch 10, Loss: 0.002605070360004902\n",
      "Epoch 10, Loss: 0.031180664896965027\n",
      "Epoch 10, Loss: 0.011545290239155293\n",
      "Epoch 10, Loss: 0.017344016581773758\n",
      "Epoch 10, Loss: 0.013404869474470615\n",
      "Epoch 10, Loss: 0.04399215802550316\n",
      "Epoch 10, Loss: 0.01582135260105133\n",
      "Epoch 10, Loss: 0.011792837642133236\n",
      "Epoch 10, Loss: 0.04579764977097511\n",
      "Epoch 10, Loss: 0.0031617998611181974\n",
      "Epoch 10, Loss: 0.042231421917676926\n",
      "Epoch 10, Loss: 0.00036660698242485523\n",
      "Epoch 10, Loss: 0.0005536146345548332\n",
      "Epoch 10, Loss: 0.001379603985697031\n",
      "Epoch 10, Loss: 0.0005193944089114666\n",
      "Epoch 10, Loss: 0.26426443457603455\n",
      "Epoch 10, Loss: 0.0009384109871461987\n",
      "Epoch 10, Loss: 0.0013986825942993164\n",
      "Epoch 10, Loss: 0.14845313131809235\n",
      "Epoch 10, Loss: 0.0026990189217031\n",
      "Epoch 10, Loss: 0.036182830048104124\n",
      "Accuracy on the test set: 98.88888888888889%\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Elephant       0.00      0.00      0.00         4\n",
      "    Elephant       0.99      1.00      0.99       356\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.49      0.50      0.50       360\n",
      "weighted avg       0.98      0.99      0.98       360\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   4]\n",
      " [  0 356]]\n",
      "Starting fold 2\n",
      "Epoch 1, Loss: 0.709985613822937\n",
      "Epoch 1, Loss: 0.22198162972927094\n",
      "Epoch 1, Loss: 0.01612135022878647\n",
      "Epoch 1, Loss: 0.0008069262257777154\n",
      "Epoch 1, Loss: 2.355145988985896e-05\n",
      "Epoch 1, Loss: 3.464517135398637e-07\n",
      "Epoch 1, Loss: 1.9278863668441772\n",
      "Epoch 1, Loss: 0.5026818513870239\n",
      "Epoch 1, Loss: 5.10364031924837e-07\n",
      "Epoch 1, Loss: 0.37063661217689514\n",
      "Epoch 1, Loss: 0.00013551549636758864\n",
      "Epoch 1, Loss: 0.0006127459346316755\n",
      "Epoch 1, Loss: 0.0018440569983795285\n",
      "Epoch 1, Loss: 0.18586048483848572\n",
      "Epoch 1, Loss: 0.013355427421629429\n",
      "Epoch 1, Loss: 0.15267829596996307\n",
      "Epoch 1, Loss: 0.12276867777109146\n",
      "Epoch 1, Loss: 0.06453613191843033\n",
      "Epoch 1, Loss: 0.0758051872253418\n",
      "Epoch 1, Loss: 0.07115744799375534\n",
      "Epoch 1, Loss: 0.060782305896282196\n",
      "Epoch 1, Loss: 0.04953509941697121\n",
      "Epoch 1, Loss: 0.03354378789663315\n",
      "Epoch 1, Loss: 0.02142632007598877\n",
      "Epoch 1, Loss: 0.01328886579722166\n",
      "Epoch 1, Loss: 0.005696037318557501\n",
      "Epoch 1, Loss: 0.0035546983126550913\n",
      "Epoch 1, Loss: 0.0017373296432197094\n",
      "Epoch 1, Loss: 0.0007350570522248745\n",
      "Epoch 1, Loss: 0.5733215808868408\n",
      "Epoch 1, Loss: 0.00045533484080806375\n",
      "Epoch 1, Loss: 0.0004694412637036294\n",
      "Epoch 1, Loss: 0.0009527169750072062\n",
      "Epoch 1, Loss: 0.0006326878210529685\n",
      "Epoch 1, Loss: 0.001283086370676756\n",
      "Epoch 1, Loss: 0.0011899207020178437\n",
      "Epoch 1, Loss: 0.20311106741428375\n",
      "Epoch 1, Loss: 0.0013431344414129853\n",
      "Epoch 1, Loss: 0.0021474938839673996\n",
      "Epoch 1, Loss: 0.0025839286390691996\n",
      "Epoch 1, Loss: 0.0024540384765714407\n",
      "Epoch 1, Loss: 0.17807309329509735\n",
      "Epoch 1, Loss: 0.18306873738765717\n",
      "Epoch 1, Loss: 0.16440488398075104\n",
      "Epoch 1, Loss: 0.009459560737013817\n",
      "Epoch 1, Loss: 0.13231397641742812\n",
      "Epoch 2, Loss: 0.15397515892982483\n",
      "Epoch 2, Loss: 0.01991010084748268\n",
      "Epoch 2, Loss: 0.1410093456506729\n",
      "Epoch 2, Loss: 0.02923981286585331\n",
      "Epoch 2, Loss: 0.03333987295627594\n",
      "Epoch 2, Loss: 0.035486601293087006\n",
      "Epoch 2, Loss: 0.14086733758449554\n",
      "Epoch 2, Loss: 0.027790000662207603\n",
      "Epoch 2, Loss: 0.12856896221637726\n",
      "Epoch 2, Loss: 0.024859687313437462\n",
      "Epoch 2, Loss: 0.02195560932159424\n",
      "Epoch 2, Loss: 0.1362282931804657\n",
      "Epoch 2, Loss: 0.14482253789901733\n",
      "Epoch 2, Loss: 0.2659584879875183\n",
      "Epoch 2, Loss: 0.013860166072845459\n",
      "Epoch 2, Loss: 0.2720130681991577\n",
      "Epoch 2, Loss: 0.014504341408610344\n",
      "Epoch 2, Loss: 0.016066089272499084\n",
      "Epoch 2, Loss: 0.019620541483163834\n",
      "Epoch 2, Loss: 0.01574568636715412\n",
      "Epoch 2, Loss: 0.015317642129957676\n",
      "Epoch 2, Loss: 0.12411212176084518\n",
      "Epoch 2, Loss: 0.1480170488357544\n",
      "Epoch 2, Loss: 0.013281389139592648\n",
      "Epoch 2, Loss: 0.15541784465312958\n",
      "Epoch 2, Loss: 0.013374695554375648\n",
      "Epoch 2, Loss: 0.014182992279529572\n",
      "Epoch 2, Loss: 0.013503167778253555\n",
      "Epoch 2, Loss: 0.01266187708824873\n",
      "Epoch 2, Loss: 0.010403585620224476\n",
      "Epoch 2, Loss: 0.1667110174894333\n",
      "Epoch 2, Loss: 0.13768792152404785\n",
      "Epoch 2, Loss: 0.006536732893437147\n",
      "Epoch 2, Loss: 0.007696322165429592\n",
      "Epoch 2, Loss: 0.008816382847726345\n",
      "Epoch 2, Loss: 0.008366046473383904\n",
      "Epoch 2, Loss: 0.007737023290246725\n",
      "Epoch 2, Loss: 0.007122832350432873\n",
      "Epoch 2, Loss: 0.005555845331400633\n",
      "Epoch 2, Loss: 0.17783553898334503\n",
      "Epoch 2, Loss: 0.004734004382044077\n",
      "Epoch 2, Loss: 0.0048926472663879395\n",
      "Epoch 2, Loss: 0.004692413378506899\n",
      "Epoch 2, Loss: 0.004915307275950909\n",
      "Epoch 2, Loss: 0.003102826653048396\n",
      "Epoch 2, Loss: 0.06072215397014386\n",
      "Epoch 3, Loss: 0.00251280190423131\n",
      "Epoch 3, Loss: 0.0028949682600796223\n",
      "Epoch 3, Loss: 0.0013987114652991295\n",
      "Epoch 3, Loss: 0.2272901087999344\n",
      "Epoch 3, Loss: 0.0012680237414315343\n",
      "Epoch 3, Loss: 0.002019173000007868\n",
      "Epoch 3, Loss: 0.0020735214930027723\n",
      "Epoch 3, Loss: 0.0030151328537613153\n",
      "Epoch 3, Loss: 0.1810060739517212\n",
      "Epoch 3, Loss: 0.16250433027744293\n",
      "Epoch 3, Loss: 0.005157706327736378\n",
      "Epoch 3, Loss: 0.008910457603633404\n",
      "Epoch 3, Loss: 0.010916502214968204\n",
      "Epoch 3, Loss: 0.013034767471253872\n",
      "Epoch 3, Loss: 0.015810495242476463\n",
      "Epoch 3, Loss: 0.14747177064418793\n",
      "Epoch 3, Loss: 0.01636006124317646\n",
      "Epoch 3, Loss: 0.13147111237049103\n",
      "Epoch 3, Loss: 0.1276426613330841\n",
      "Epoch 3, Loss: 0.2564605474472046\n",
      "Epoch 3, Loss: 0.0247791800647974\n",
      "Epoch 3, Loss: 0.025916889309883118\n",
      "Epoch 3, Loss: 0.11925309151411057\n",
      "Epoch 3, Loss: 0.03336767479777336\n",
      "Epoch 3, Loss: 0.12544269859790802\n",
      "Epoch 3, Loss: 0.025234686210751534\n",
      "Epoch 3, Loss: 0.019645486027002335\n",
      "Epoch 3, Loss: 0.13487040996551514\n",
      "Epoch 3, Loss: 0.015568974427878857\n",
      "Epoch 3, Loss: 0.011159108951687813\n",
      "Epoch 3, Loss: 0.008313622325658798\n",
      "Epoch 3, Loss: 0.2854377031326294\n",
      "Epoch 3, Loss: 0.14823830127716064\n",
      "Epoch 3, Loss: 0.006930782459676266\n",
      "Epoch 3, Loss: 0.008229079656302929\n",
      "Epoch 3, Loss: 0.007520473096519709\n",
      "Epoch 3, Loss: 0.010396232828497887\n",
      "Epoch 3, Loss: 0.008547651581466198\n",
      "Epoch 3, Loss: 0.009088290855288506\n",
      "Epoch 3, Loss: 0.0061189779080450535\n",
      "Epoch 3, Loss: 0.16614235937595367\n",
      "Epoch 3, Loss: 0.007216387894004583\n",
      "Epoch 3, Loss: 0.004825835581868887\n",
      "Epoch 3, Loss: 0.12909330427646637\n",
      "Epoch 3, Loss: 0.0055316477082669735\n",
      "Epoch 3, Loss: 0.05924639505489419\n",
      "Epoch 4, Loss: 0.005115819163620472\n",
      "Epoch 4, Loss: 0.005462963134050369\n",
      "Epoch 4, Loss: 0.006019074469804764\n",
      "Epoch 4, Loss: 0.005127847660332918\n",
      "Epoch 4, Loss: 0.003408710705116391\n",
      "Epoch 4, Loss: 0.005713372025638819\n",
      "Epoch 4, Loss: 0.317859023809433\n",
      "Epoch 4, Loss: 0.126225546002388\n",
      "Epoch 4, Loss: 0.1364080309867859\n",
      "Epoch 4, Loss: 0.23489698767662048\n",
      "Epoch 4, Loss: 0.0188226867467165\n",
      "Epoch 4, Loss: 0.12223565578460693\n",
      "Epoch 4, Loss: 0.13581447303295135\n",
      "Epoch 4, Loss: 0.05265276879072189\n",
      "Epoch 4, Loss: 0.05219004675745964\n",
      "Epoch 4, Loss: 0.12798848748207092\n",
      "Epoch 4, Loss: 0.04918726161122322\n",
      "Epoch 4, Loss: 0.03363138809800148\n",
      "Epoch 4, Loss: 0.02579403668642044\n",
      "Epoch 4, Loss: 0.013735487125813961\n",
      "Epoch 4, Loss: 0.0062538194470107555\n",
      "Epoch 4, Loss: 0.0046742623671889305\n",
      "Epoch 4, Loss: 0.004566803574562073\n",
      "Epoch 4, Loss: 0.0026562241837382317\n",
      "Epoch 4, Loss: 0.23821674287319183\n",
      "Epoch 4, Loss: 0.000629399495664984\n",
      "Epoch 4, Loss: 0.0006315858336165547\n",
      "Epoch 4, Loss: 0.21912409365177155\n",
      "Epoch 4, Loss: 0.0008697445737197995\n",
      "Epoch 4, Loss: 0.15613214671611786\n",
      "Epoch 4, Loss: 0.37128645181655884\n",
      "Epoch 4, Loss: 0.0049708629958331585\n",
      "Epoch 4, Loss: 0.13552913069725037\n",
      "Epoch 4, Loss: 0.015319646336138248\n",
      "Epoch 4, Loss: 0.02381645329296589\n",
      "Epoch 4, Loss: 0.122840017080307\n",
      "Epoch 4, Loss: 0.042204707860946655\n",
      "Epoch 4, Loss: 0.046538159251213074\n",
      "Epoch 4, Loss: 0.04487540200352669\n",
      "Epoch 4, Loss: 0.03053152561187744\n",
      "Epoch 4, Loss: 0.027954498305916786\n",
      "Epoch 4, Loss: 0.018523942679166794\n",
      "Epoch 4, Loss: 0.013838551938533783\n",
      "Epoch 4, Loss: 0.007189569529145956\n",
      "Epoch 4, Loss: 0.004685218911617994\n",
      "Epoch 4, Loss: 0.06715885841727463\n",
      "Epoch 5, Loss: 0.0032866003457456827\n",
      "Epoch 5, Loss: 0.0019393221009522676\n",
      "Epoch 5, Loss: 0.3586779236793518\n",
      "Epoch 5, Loss: 0.0009027430205605924\n",
      "Epoch 5, Loss: 0.0015653626760467887\n",
      "Epoch 5, Loss: 0.0015158524038270116\n",
      "Epoch 5, Loss: 0.0010880486806854606\n",
      "Epoch 5, Loss: 0.19415797293186188\n",
      "Epoch 5, Loss: 0.0013453643769025803\n",
      "Epoch 5, Loss: 0.0021328269504010677\n",
      "Epoch 5, Loss: 0.0020650397054851055\n",
      "Epoch 5, Loss: 0.003075285814702511\n",
      "Epoch 5, Loss: 0.16927364468574524\n",
      "Epoch 5, Loss: 0.004664548207074404\n",
      "Epoch 5, Loss: 0.005384120624512434\n",
      "Epoch 5, Loss: 0.1324475258588791\n",
      "Epoch 5, Loss: 0.006789281032979488\n",
      "Epoch 5, Loss: 0.10449188202619553\n",
      "Epoch 5, Loss: 0.013686029240489006\n",
      "Epoch 5, Loss: 0.01633850485086441\n",
      "Epoch 5, Loss: 0.2441052347421646\n",
      "Epoch 5, Loss: 0.01782851479947567\n",
      "Epoch 5, Loss: 0.10036290436983109\n",
      "Epoch 5, Loss: 0.21507440507411957\n",
      "Epoch 5, Loss: 0.02987465262413025\n",
      "Epoch 5, Loss: 0.034743569791316986\n",
      "Epoch 5, Loss: 0.030521081760525703\n",
      "Epoch 5, Loss: 0.028089366853237152\n",
      "Epoch 5, Loss: 0.09522861987352371\n",
      "Epoch 5, Loss: 0.018840642645955086\n",
      "Epoch 5, Loss: 0.010590385645627975\n",
      "Epoch 5, Loss: 0.013851936906576157\n",
      "Epoch 5, Loss: 0.006514555774629116\n",
      "Epoch 5, Loss: 0.12022202461957932\n",
      "Epoch 5, Loss: 0.005485262256115675\n",
      "Epoch 5, Loss: 0.0033679595217108727\n",
      "Epoch 5, Loss: 0.0018335357308387756\n",
      "Epoch 5, Loss: 0.0013617316726595163\n",
      "Epoch 5, Loss: 0.1333608627319336\n",
      "Epoch 5, Loss: 0.000699652242474258\n",
      "Epoch 5, Loss: 0.21407252550125122\n",
      "Epoch 5, Loss: 0.0023869452998042107\n",
      "Epoch 5, Loss: 0.0021750740706920624\n",
      "Epoch 5, Loss: 0.005553625524044037\n",
      "Epoch 5, Loss: 0.1323910653591156\n",
      "Epoch 5, Loss: 0.05540808921343544\n",
      "Epoch 6, Loss: 0.005680128000676632\n",
      "Epoch 6, Loss: 0.006760945077985525\n",
      "Epoch 6, Loss: 0.008333361707627773\n",
      "Epoch 6, Loss: 0.009343444369733334\n",
      "Epoch 6, Loss: 0.006110773887485266\n",
      "Epoch 6, Loss: 0.006336465012282133\n",
      "Epoch 6, Loss: 0.006913407240062952\n",
      "Epoch 6, Loss: 0.005514439661055803\n",
      "Epoch 6, Loss: 0.007427220232784748\n",
      "Epoch 6, Loss: 0.00498341815546155\n",
      "Epoch 6, Loss: 0.00494661508128047\n",
      "Epoch 6, Loss: 0.0030213065911084414\n",
      "Epoch 6, Loss: 0.0029789679683744907\n",
      "Epoch 6, Loss: 0.0891239270567894\n",
      "Epoch 6, Loss: 0.21172434091567993\n",
      "Epoch 6, Loss: 0.002796263201162219\n",
      "Epoch 6, Loss: 0.0045876577496528625\n",
      "Epoch 6, Loss: 0.0025827190838754177\n",
      "Epoch 6, Loss: 0.0037815626710653305\n",
      "Epoch 6, Loss: 0.004959896206855774\n",
      "Epoch 6, Loss: 0.0037044319324195385\n",
      "Epoch 6, Loss: 0.10400326550006866\n",
      "Epoch 6, Loss: 0.08757301419973373\n",
      "Epoch 6, Loss: 0.11895372718572617\n",
      "Epoch 6, Loss: 0.0733553096652031\n",
      "Epoch 6, Loss: 0.01560472883284092\n",
      "Epoch 6, Loss: 0.024947278201580048\n",
      "Epoch 6, Loss: 0.09309495985507965\n",
      "Epoch 6, Loss: 0.07796307653188705\n",
      "Epoch 6, Loss: 0.13656820356845856\n",
      "Epoch 6, Loss: 0.1408829540014267\n",
      "Epoch 6, Loss: 0.023026714101433754\n",
      "Epoch 6, Loss: 0.021396081894636154\n",
      "Epoch 6, Loss: 0.11325997114181519\n",
      "Epoch 6, Loss: 0.07399909943342209\n",
      "Epoch 6, Loss: 0.08729293942451477\n",
      "Epoch 6, Loss: 0.027579452842473984\n",
      "Epoch 6, Loss: 0.013062406331300735\n",
      "Epoch 6, Loss: 0.008028071373701096\n",
      "Epoch 6, Loss: 0.005015248898416758\n",
      "Epoch 6, Loss: 0.17622393369674683\n",
      "Epoch 6, Loss: 0.1401292085647583\n",
      "Epoch 6, Loss: 0.008199361152946949\n",
      "Epoch 6, Loss: 0.10573899745941162\n",
      "Epoch 6, Loss: 0.0160097386687994\n",
      "Epoch 6, Loss: 0.046522645207328926\n",
      "Epoch 7, Loss: 0.07776333391666412\n",
      "Epoch 7, Loss: 0.021160637959837914\n",
      "Epoch 7, Loss: 0.018390057608485222\n",
      "Epoch 7, Loss: 0.013276978395879269\n",
      "Epoch 7, Loss: 0.14981433749198914\n",
      "Epoch 7, Loss: 0.01843968592584133\n",
      "Epoch 7, Loss: 0.10442366451025009\n",
      "Epoch 7, Loss: 0.01686187833547592\n",
      "Epoch 7, Loss: 0.006818422116339207\n",
      "Epoch 7, Loss: 0.011720835231244564\n",
      "Epoch 7, Loss: 0.11811897903680801\n",
      "Epoch 7, Loss: 0.005229763220995665\n",
      "Epoch 7, Loss: 0.011682098731398582\n",
      "Epoch 7, Loss: 0.06877636909484863\n",
      "Epoch 7, Loss: 0.01391910295933485\n",
      "Epoch 7, Loss: 0.12008221447467804\n",
      "Epoch 7, Loss: 0.005569199565798044\n",
      "Epoch 7, Loss: 0.007388101890683174\n",
      "Epoch 7, Loss: 0.007880115881562233\n",
      "Epoch 7, Loss: 0.009845374152064323\n",
      "Epoch 7, Loss: 0.007451038341969252\n",
      "Epoch 7, Loss: 0.12135306000709534\n",
      "Epoch 7, Loss: 0.004931432660669088\n",
      "Epoch 7, Loss: 0.014548409730196\n",
      "Epoch 7, Loss: 0.001350686769001186\n",
      "Epoch 7, Loss: 0.0016198038356378675\n",
      "Epoch 7, Loss: 0.002388396766036749\n",
      "Epoch 7, Loss: 0.0033504185266792774\n",
      "Epoch 7, Loss: 0.003451696364209056\n",
      "Epoch 7, Loss: 0.0006797700189054012\n",
      "Epoch 7, Loss: 0.13398705422878265\n",
      "Epoch 7, Loss: 0.0011108876205980778\n",
      "Epoch 7, Loss: 0.4288499057292938\n",
      "Epoch 7, Loss: 0.004715308081358671\n",
      "Epoch 7, Loss: 0.00486517371609807\n",
      "Epoch 7, Loss: 0.016094792634248734\n",
      "Epoch 7, Loss: 0.031636957079172134\n",
      "Epoch 7, Loss: 0.13952121138572693\n",
      "Epoch 7, Loss: 0.03387681394815445\n",
      "Epoch 7, Loss: 0.07528277486562729\n",
      "Epoch 7, Loss: 0.06495308876037598\n",
      "Epoch 7, Loss: 0.02777690440416336\n",
      "Epoch 7, Loss: 0.012177466414868832\n",
      "Epoch 7, Loss: 0.009914839640259743\n",
      "Epoch 7, Loss: 0.007473577745258808\n",
      "Epoch 7, Loss: 0.04356716932832367\n",
      "Epoch 8, Loss: 0.003538619726896286\n",
      "Epoch 8, Loss: 0.001372142811305821\n",
      "Epoch 8, Loss: 0.0006736503564752638\n",
      "Epoch 8, Loss: 8.122334838844836e-05\n",
      "Epoch 8, Loss: 0.3240756392478943\n",
      "Epoch 8, Loss: 0.22404006123542786\n",
      "Epoch 8, Loss: 0.00020448729628697038\n",
      "Epoch 8, Loss: 0.00050910166464746\n",
      "Epoch 8, Loss: 0.0009753997437655926\n",
      "Epoch 8, Loss: 0.11381049454212189\n",
      "Epoch 8, Loss: 0.007619330659508705\n",
      "Epoch 8, Loss: 0.09415381401777267\n",
      "Epoch 8, Loss: 0.008487515151500702\n",
      "Epoch 8, Loss: 0.016452476382255554\n",
      "Epoch 8, Loss: 0.05465058982372284\n",
      "Epoch 8, Loss: 0.02673053741455078\n",
      "Epoch 8, Loss: 0.06484449654817581\n",
      "Epoch 8, Loss: 0.021031655371189117\n",
      "Epoch 8, Loss: 0.02111693285405636\n",
      "Epoch 8, Loss: 0.024710116907954216\n",
      "Epoch 8, Loss: 0.0982041135430336\n",
      "Epoch 8, Loss: 0.0437166728079319\n",
      "Epoch 8, Loss: 0.010626161471009254\n",
      "Epoch 8, Loss: 0.007620622403919697\n",
      "Epoch 8, Loss: 0.09657354652881622\n",
      "Epoch 8, Loss: 0.00527680991217494\n",
      "Epoch 8, Loss: 0.13644112646579742\n",
      "Epoch 8, Loss: 0.002540546003729105\n",
      "Epoch 8, Loss: 0.003054671688005328\n",
      "Epoch 8, Loss: 0.07767130434513092\n",
      "Epoch 8, Loss: 0.04476211592555046\n",
      "Epoch 8, Loss: 0.08464281260967255\n",
      "Epoch 8, Loss: 0.023221323266625404\n",
      "Epoch 8, Loss: 0.017798740416765213\n",
      "Epoch 8, Loss: 0.03864676505327225\n",
      "Epoch 8, Loss: 0.009027819149196148\n",
      "Epoch 8, Loss: 0.01179371029138565\n",
      "Epoch 8, Loss: 0.0031246754806488752\n",
      "Epoch 8, Loss: 0.016850972548127174\n",
      "Epoch 8, Loss: 0.006606058217585087\n",
      "Epoch 8, Loss: 0.005339983385056257\n",
      "Epoch 8, Loss: 0.37916100025177\n",
      "Epoch 8, Loss: 0.1541551649570465\n",
      "Epoch 8, Loss: 0.0020765550434589386\n",
      "Epoch 8, Loss: 0.005479495041072369\n",
      "Epoch 8, Loss: 0.05096646782023729\n",
      "Epoch 9, Loss: 0.0038556414656341076\n",
      "Epoch 9, Loss: 0.009421741589903831\n",
      "Epoch 9, Loss: 0.01246099453419447\n",
      "Epoch 9, Loss: 0.043101295828819275\n",
      "Epoch 9, Loss: 0.08933486044406891\n",
      "Epoch 9, Loss: 0.06254517287015915\n",
      "Epoch 9, Loss: 0.024886073544621468\n",
      "Epoch 9, Loss: 0.012773300521075726\n",
      "Epoch 9, Loss: 0.06932663917541504\n",
      "Epoch 9, Loss: 0.046787697821855545\n",
      "Epoch 9, Loss: 0.06026483699679375\n",
      "Epoch 9, Loss: 0.022240541875362396\n",
      "Epoch 9, Loss: 0.00933118350803852\n",
      "Epoch 9, Loss: 0.004969791043549776\n",
      "Epoch 9, Loss: 0.00755151454359293\n",
      "Epoch 9, Loss: 0.005283094476908445\n",
      "Epoch 9, Loss: 0.0052331904880702496\n",
      "Epoch 9, Loss: 0.0025986696127802134\n",
      "Epoch 9, Loss: 0.0009543058695271611\n",
      "Epoch 9, Loss: 0.0007886123494245112\n",
      "Epoch 9, Loss: 0.2124827802181244\n",
      "Epoch 9, Loss: 0.0006974716088734567\n",
      "Epoch 9, Loss: 0.0009511802927590907\n",
      "Epoch 9, Loss: 0.001118019106797874\n",
      "Epoch 9, Loss: 0.0007937430054880679\n",
      "Epoch 9, Loss: 0.38849565386772156\n",
      "Epoch 9, Loss: 0.001045836484991014\n",
      "Epoch 9, Loss: 0.04812444746494293\n",
      "Epoch 9, Loss: 0.004472677130252123\n",
      "Epoch 9, Loss: 0.007261231075972319\n",
      "Epoch 9, Loss: 0.0474730022251606\n",
      "Epoch 9, Loss: 0.03127274289727211\n",
      "Epoch 9, Loss: 0.01589442975819111\n",
      "Epoch 9, Loss: 0.03141562268137932\n",
      "Epoch 9, Loss: 0.020836610347032547\n",
      "Epoch 9, Loss: 0.029055431485176086\n",
      "Epoch 9, Loss: 0.010133178904652596\n",
      "Epoch 9, Loss: 0.010675307363271713\n",
      "Epoch 9, Loss: 0.037627629935741425\n",
      "Epoch 9, Loss: 0.00431817676872015\n",
      "Epoch 9, Loss: 0.003789689391851425\n",
      "Epoch 9, Loss: 0.001726621761918068\n",
      "Epoch 9, Loss: 0.005445412360131741\n",
      "Epoch 9, Loss: 0.004469148814678192\n",
      "Epoch 9, Loss: 0.2381187081336975\n",
      "Epoch 9, Loss: 0.03669786470321317\n",
      "Epoch 10, Loss: 0.09889225661754608\n",
      "Epoch 10, Loss: 0.003311884356662631\n",
      "Epoch 10, Loss: 0.06418293714523315\n",
      "Epoch 10, Loss: 0.0024108299985527992\n",
      "Epoch 10, Loss: 0.1010686382651329\n",
      "Epoch 10, Loss: 0.003460359526798129\n",
      "Epoch 10, Loss: 0.004186437930911779\n",
      "Epoch 10, Loss: 0.00885361060500145\n",
      "Epoch 10, Loss: 0.07639022916555405\n",
      "Epoch 10, Loss: 0.03180127590894699\n",
      "Epoch 10, Loss: 0.03410910815000534\n",
      "Epoch 10, Loss: 0.04307490587234497\n",
      "Epoch 10, Loss: 0.047218985855579376\n",
      "Epoch 10, Loss: 0.016030805185437202\n",
      "Epoch 10, Loss: 0.007291377522051334\n",
      "Epoch 10, Loss: 0.042059410363435745\n",
      "Epoch 10, Loss: 0.017429370433092117\n",
      "Epoch 10, Loss: 0.04942620173096657\n",
      "Epoch 10, Loss: 0.008432398550212383\n",
      "Epoch 10, Loss: 0.0031039745081216097\n",
      "Epoch 10, Loss: 0.004771403968334198\n",
      "Epoch 10, Loss: 0.11555050313472748\n",
      "Epoch 10, Loss: 0.0006653639720752835\n",
      "Epoch 10, Loss: 0.028277883306145668\n",
      "Epoch 10, Loss: 0.001111599849537015\n",
      "Epoch 10, Loss: 0.12303130328655243\n",
      "Epoch 10, Loss: 0.003060090122744441\n",
      "Epoch 10, Loss: 0.002697443589568138\n",
      "Epoch 10, Loss: 0.0012351819314062595\n",
      "Epoch 10, Loss: 0.08488450944423676\n",
      "Epoch 10, Loss: 0.005902749486267567\n",
      "Epoch 10, Loss: 0.0069269053637981415\n",
      "Epoch 10, Loss: 0.01888168603181839\n",
      "Epoch 10, Loss: 0.017059680074453354\n",
      "Epoch 10, Loss: 0.030896930024027824\n",
      "Epoch 10, Loss: 0.004979067016392946\n",
      "Epoch 10, Loss: 0.03097030147910118\n",
      "Epoch 10, Loss: 0.0042404537089169025\n",
      "Epoch 10, Loss: 0.009708906523883343\n",
      "Epoch 10, Loss: 0.0018156090518459678\n",
      "Epoch 10, Loss: 0.00188160571269691\n",
      "Epoch 10, Loss: 0.0023900580126792192\n",
      "Epoch 10, Loss: 0.003473043441772461\n",
      "Epoch 10, Loss: 0.03246588632464409\n",
      "Epoch 10, Loss: 0.0012088611256331205\n",
      "Epoch 10, Loss: 0.026684933859441016\n",
      "Accuracy on the test set: 98.88888888888889%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Elephant       0.00      0.00      0.00         4\n",
      "    Elephant       0.99      1.00      0.99       356\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.49      0.50      0.50       360\n",
      "weighted avg       0.98      0.99      0.98       360\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   4]\n",
      " [  0 356]]\n",
      "Starting fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6932142972946167\n",
      "Epoch 1, Loss: 0.2723560333251953\n",
      "Epoch 1, Loss: 0.04337079077959061\n",
      "Epoch 1, Loss: 0.20579810440540314\n",
      "Epoch 1, Loss: 0.0005400777445174754\n",
      "Epoch 1, Loss: 0.00010845762881217524\n",
      "Epoch 1, Loss: 1.9717092072824016e-05\n",
      "Epoch 1, Loss: 7.02209126757225e-06\n",
      "Epoch 1, Loss: 0.4764867126941681\n",
      "Epoch 1, Loss: 4.540860118140699e-06\n",
      "Epoch 1, Loss: 6.99962220096495e-06\n",
      "Epoch 1, Loss: 0.7275425791740417\n",
      "Epoch 1, Loss: 8.845605771057308e-05\n",
      "Epoch 1, Loss: 0.3282538056373596\n",
      "Epoch 1, Loss: 0.20424197614192963\n",
      "Epoch 1, Loss: 0.003546477295458317\n",
      "Epoch 1, Loss: 0.009048271924257278\n",
      "Epoch 1, Loss: 0.021714622154831886\n",
      "Epoch 1, Loss: 0.028473839163780212\n",
      "Epoch 1, Loss: 0.1313093602657318\n",
      "Epoch 1, Loss: 0.14086773991584778\n",
      "Epoch 1, Loss: 0.05083353444933891\n",
      "Epoch 1, Loss: 0.13571566343307495\n",
      "Epoch 1, Loss: 0.04633050411939621\n",
      "Epoch 1, Loss: 0.04050522670149803\n",
      "Epoch 1, Loss: 0.029756663367152214\n",
      "Epoch 1, Loss: 0.02161243185400963\n",
      "Epoch 1, Loss: 0.1365460604429245\n",
      "Epoch 1, Loss: 0.1460406333208084\n",
      "Epoch 1, Loss: 0.006990691181272268\n",
      "Epoch 1, Loss: 0.004490556661039591\n",
      "Epoch 1, Loss: 0.003310389583930373\n",
      "Epoch 1, Loss: 0.19650998711585999\n",
      "Epoch 1, Loss: 0.1871286928653717\n",
      "Epoch 1, Loss: 0.1915006935596466\n",
      "Epoch 1, Loss: 0.003937472589313984\n",
      "Epoch 1, Loss: 0.007109599653631449\n",
      "Epoch 1, Loss: 0.009822649881243706\n",
      "Epoch 1, Loss: 0.13373297452926636\n",
      "Epoch 1, Loss: 0.012202661484479904\n",
      "Epoch 1, Loss: 0.014644524082541466\n",
      "Epoch 1, Loss: 0.017457790672779083\n",
      "Epoch 1, Loss: 0.14118002355098724\n",
      "Epoch 1, Loss: 0.017810607329010963\n",
      "Epoch 1, Loss: 0.01321838703006506\n",
      "Epoch 1, Loss: 0.10789751779394566\n",
      "Epoch 2, Loss: 0.12894415855407715\n",
      "Epoch 2, Loss: 0.015186462551355362\n",
      "Epoch 2, Loss: 0.01533407624810934\n",
      "Epoch 2, Loss: 0.14078956842422485\n",
      "Epoch 2, Loss: 0.010892791673541069\n",
      "Epoch 2, Loss: 0.14516940712928772\n",
      "Epoch 2, Loss: 0.0102834552526474\n",
      "Epoch 2, Loss: 0.010289895348250866\n",
      "Epoch 2, Loss: 0.13819728791713715\n",
      "Epoch 2, Loss: 0.1381906270980835\n",
      "Epoch 2, Loss: 0.011576540768146515\n",
      "Epoch 2, Loss: 0.2819876968860626\n",
      "Epoch 2, Loss: 0.0146515853703022\n",
      "Epoch 2, Loss: 0.017250077798962593\n",
      "Epoch 2, Loss: 0.01953257992863655\n",
      "Epoch 2, Loss: 0.01762070134282112\n",
      "Epoch 2, Loss: 0.01826515793800354\n",
      "Epoch 2, Loss: 0.012944553047418594\n",
      "Epoch 2, Loss: 0.13372351229190826\n",
      "Epoch 2, Loss: 0.008435612544417381\n",
      "Epoch 2, Loss: 0.0072539313696324825\n",
      "Epoch 2, Loss: 0.00478333979845047\n",
      "Epoch 2, Loss: 0.14462441205978394\n",
      "Epoch 2, Loss: 0.005415576044470072\n",
      "Epoch 2, Loss: 0.0035505350679159164\n",
      "Epoch 2, Loss: 0.00421667005866766\n",
      "Epoch 2, Loss: 0.0037335436791181564\n",
      "Epoch 2, Loss: 0.19265547394752502\n",
      "Epoch 2, Loss: 0.15488827228546143\n",
      "Epoch 2, Loss: 0.004871230106800795\n",
      "Epoch 2, Loss: 0.006444999016821384\n",
      "Epoch 2, Loss: 0.00705584604293108\n",
      "Epoch 2, Loss: 0.009067494422197342\n",
      "Epoch 2, Loss: 0.16320489346981049\n",
      "Epoch 2, Loss: 0.012815982103347778\n",
      "Epoch 2, Loss: 0.1331956684589386\n",
      "Epoch 2, Loss: 0.14648979902267456\n",
      "Epoch 2, Loss: 0.016996655613183975\n",
      "Epoch 2, Loss: 0.019225867465138435\n",
      "Epoch 2, Loss: 0.23245282471179962\n",
      "Epoch 2, Loss: 0.016543135046958923\n",
      "Epoch 2, Loss: 0.027731290087103844\n",
      "Epoch 2, Loss: 0.024737123399972916\n",
      "Epoch 2, Loss: 0.020688774064183235\n",
      "Epoch 2, Loss: 0.012447468005120754\n",
      "Epoch 2, Loss: 0.05920792341025339\n",
      "Epoch 3, Loss: 0.009586567059159279\n",
      "Epoch 3, Loss: 0.0047296578995883465\n",
      "Epoch 3, Loss: 0.0028308320324867964\n",
      "Epoch 3, Loss: 0.0015495504485443234\n",
      "Epoch 3, Loss: 0.5632978677749634\n",
      "Epoch 3, Loss: 0.0012977104634046555\n",
      "Epoch 3, Loss: 0.0030488334596157074\n",
      "Epoch 3, Loss: 0.003982034511864185\n",
      "Epoch 3, Loss: 0.005657876841723919\n",
      "Epoch 3, Loss: 0.0059554497711360455\n",
      "Epoch 3, Loss: 0.13567058742046356\n",
      "Epoch 3, Loss: 0.13954846560955048\n",
      "Epoch 3, Loss: 0.012651353143155575\n",
      "Epoch 3, Loss: 0.013348715379834175\n",
      "Epoch 3, Loss: 0.019363416358828545\n",
      "Epoch 3, Loss: 0.018500013276934624\n",
      "Epoch 3, Loss: 0.018932171165943146\n",
      "Epoch 3, Loss: 0.013810726813971996\n",
      "Epoch 3, Loss: 0.013367420993745327\n",
      "Epoch 3, Loss: 0.12819498777389526\n",
      "Epoch 3, Loss: 0.2923021912574768\n",
      "Epoch 3, Loss: 0.1290666162967682\n",
      "Epoch 3, Loss: 0.014681439846754074\n",
      "Epoch 3, Loss: 0.017028475180268288\n",
      "Epoch 3, Loss: 0.11690646409988403\n",
      "Epoch 3, Loss: 0.015414822846651077\n",
      "Epoch 3, Loss: 0.0192672461271286\n",
      "Epoch 3, Loss: 0.013263313099741936\n",
      "Epoch 3, Loss: 0.009880132041871548\n",
      "Epoch 3, Loss: 0.01162145659327507\n",
      "Epoch 3, Loss: 0.14300177991390228\n",
      "Epoch 3, Loss: 0.007243565283715725\n",
      "Epoch 3, Loss: 0.008049940690398216\n",
      "Epoch 3, Loss: 0.15562783181667328\n",
      "Epoch 3, Loss: 0.0075576426461339\n",
      "Epoch 3, Loss: 0.005267968866974115\n",
      "Epoch 3, Loss: 0.12800376117229462\n",
      "Epoch 3, Loss: 0.12847287952899933\n",
      "Epoch 3, Loss: 0.010031415149569511\n",
      "Epoch 3, Loss: 0.10950959473848343\n",
      "Epoch 3, Loss: 0.014914401806890965\n",
      "Epoch 3, Loss: 0.013219664804637432\n",
      "Epoch 3, Loss: 0.019521692767739296\n",
      "Epoch 3, Loss: 0.013737097382545471\n",
      "Epoch 3, Loss: 0.2746453583240509\n",
      "Epoch 3, Loss: 0.062079133121814165\n",
      "Epoch 4, Loss: 0.12495965510606766\n",
      "Epoch 4, Loss: 0.014832277782261372\n",
      "Epoch 4, Loss: 0.018838007003068924\n",
      "Epoch 4, Loss: 0.1060744971036911\n",
      "Epoch 4, Loss: 0.1016872450709343\n",
      "Epoch 4, Loss: 0.022590624168515205\n",
      "Epoch 4, Loss: 0.02051548659801483\n",
      "Epoch 4, Loss: 0.016294017434120178\n",
      "Epoch 4, Loss: 0.10871943831443787\n",
      "Epoch 4, Loss: 0.11503072082996368\n",
      "Epoch 4, Loss: 0.012379697524011135\n",
      "Epoch 4, Loss: 0.011662464588880539\n",
      "Epoch 4, Loss: 0.008960984647274017\n",
      "Epoch 4, Loss: 0.005222217179834843\n",
      "Epoch 4, Loss: 0.005376544781029224\n",
      "Epoch 4, Loss: 0.17164668440818787\n",
      "Epoch 4, Loss: 0.11375241726636887\n",
      "Epoch 4, Loss: 0.0046380371786653996\n",
      "Epoch 4, Loss: 0.007607709616422653\n",
      "Epoch 4, Loss: 0.005331732332706451\n",
      "Epoch 4, Loss: 0.009978017769753933\n",
      "Epoch 4, Loss: 0.007025294005870819\n",
      "Epoch 4, Loss: 0.003938139881938696\n",
      "Epoch 4, Loss: 0.004082094877958298\n",
      "Epoch 4, Loss: 0.005929046310484409\n",
      "Epoch 4, Loss: 0.35253554582595825\n",
      "Epoch 4, Loss: 0.09526010602712631\n",
      "Epoch 4, Loss: 0.19057253003120422\n",
      "Epoch 4, Loss: 0.014030545018613338\n",
      "Epoch 4, Loss: 0.02336859703063965\n",
      "Epoch 4, Loss: 0.030778225511312485\n",
      "Epoch 4, Loss: 0.031024985015392303\n",
      "Epoch 4, Loss: 0.12795670330524445\n",
      "Epoch 4, Loss: 0.039680708199739456\n",
      "Epoch 4, Loss: 0.023245064541697502\n",
      "Epoch 4, Loss: 0.12751713395118713\n",
      "Epoch 4, Loss: 0.12482060492038727\n",
      "Epoch 4, Loss: 0.010428025387227535\n",
      "Epoch 4, Loss: 0.011002536863088608\n",
      "Epoch 4, Loss: 0.11292050778865814\n",
      "Epoch 4, Loss: 0.006873852573335171\n",
      "Epoch 4, Loss: 0.0054206945933401585\n",
      "Epoch 4, Loss: 0.004851816222071648\n",
      "Epoch 4, Loss: 0.004535511136054993\n",
      "Epoch 4, Loss: 0.003427188377827406\n",
      "Epoch 4, Loss: 0.05260719853556818\n",
      "Epoch 5, Loss: 0.4622417986392975\n",
      "Epoch 5, Loss: 0.004395675379782915\n",
      "Epoch 5, Loss: 0.005836946424096823\n",
      "Epoch 5, Loss: 0.006023650523275137\n",
      "Epoch 5, Loss: 0.012757192365825176\n",
      "Epoch 5, Loss: 0.10697726160287857\n",
      "Epoch 5, Loss: 0.01900384947657585\n",
      "Epoch 5, Loss: 0.017063818871974945\n",
      "Epoch 5, Loss: 0.013834021985530853\n",
      "Epoch 5, Loss: 0.011730343103408813\n",
      "Epoch 5, Loss: 0.012908362783491611\n",
      "Epoch 5, Loss: 0.011476408690214157\n",
      "Epoch 5, Loss: 0.009599341079592705\n",
      "Epoch 5, Loss: 0.18875718116760254\n",
      "Epoch 5, Loss: 0.14564865827560425\n",
      "Epoch 5, Loss: 0.009666218422353268\n",
      "Epoch 5, Loss: 0.004912011791020632\n",
      "Epoch 5, Loss: 0.0105724623426795\n",
      "Epoch 5, Loss: 0.013268955983221531\n",
      "Epoch 5, Loss: 0.006636635400354862\n",
      "Epoch 5, Loss: 0.11103060096502304\n",
      "Epoch 5, Loss: 0.20654545724391937\n",
      "Epoch 5, Loss: 0.018944665789604187\n",
      "Epoch 5, Loss: 0.00882494542747736\n",
      "Epoch 5, Loss: 0.015114286914467812\n",
      "Epoch 5, Loss: 0.1342419534921646\n",
      "Epoch 5, Loss: 0.020802825689315796\n",
      "Epoch 5, Loss: 0.07915763556957245\n",
      "Epoch 5, Loss: 0.06618131697177887\n",
      "Epoch 5, Loss: 0.008684520609676838\n",
      "Epoch 5, Loss: 0.011083858087658882\n",
      "Epoch 5, Loss: 0.018555739894509315\n",
      "Epoch 5, Loss: 0.01699971966445446\n",
      "Epoch 5, Loss: 0.01186648290604353\n",
      "Epoch 5, Loss: 0.006569079589098692\n",
      "Epoch 5, Loss: 0.001907444791868329\n",
      "Epoch 5, Loss: 0.33532950282096863\n",
      "Epoch 5, Loss: 0.0017329141264781356\n",
      "Epoch 5, Loss: 0.0013761015143245459\n",
      "Epoch 5, Loss: 0.0034402909222990274\n",
      "Epoch 5, Loss: 0.0009830103954300284\n",
      "Epoch 5, Loss: 0.011994393542408943\n",
      "Epoch 5, Loss: 0.0034175170585513115\n",
      "Epoch 5, Loss: 0.06041422858834267\n",
      "Epoch 5, Loss: 0.006028523202985525\n",
      "Epoch 5, Loss: 0.049656395779715645\n",
      "Epoch 6, Loss: 0.0034612491726875305\n",
      "Epoch 6, Loss: 0.16731591522693634\n",
      "Epoch 6, Loss: 0.008300825953483582\n",
      "Epoch 6, Loss: 0.02592664398252964\n",
      "Epoch 6, Loss: 0.12522859871387482\n",
      "Epoch 6, Loss: 0.019516989588737488\n",
      "Epoch 6, Loss: 0.027208812534809113\n",
      "Epoch 6, Loss: 0.01633533090353012\n",
      "Epoch 6, Loss: 0.00426723575219512\n",
      "Epoch 6, Loss: 0.012736174277961254\n",
      "Epoch 6, Loss: 0.09871245175600052\n",
      "Epoch 6, Loss: 0.00875943899154663\n",
      "Epoch 6, Loss: 0.005151638761162758\n",
      "Epoch 6, Loss: 0.10193169862031937\n",
      "Epoch 6, Loss: 0.0688013881444931\n",
      "Epoch 6, Loss: 0.0031632808968424797\n",
      "Epoch 6, Loss: 0.13069774210453033\n",
      "Epoch 6, Loss: 0.009516746737062931\n",
      "Epoch 6, Loss: 0.009595894254744053\n",
      "Epoch 6, Loss: 0.006735809147357941\n",
      "Epoch 6, Loss: 0.003360833041369915\n",
      "Epoch 6, Loss: 0.012354464270174503\n",
      "Epoch 6, Loss: 0.0043794852681458\n",
      "Epoch 6, Loss: 0.05684607848525047\n",
      "Epoch 6, Loss: 0.0690445750951767\n",
      "Epoch 6, Loss: 0.005026569589972496\n",
      "Epoch 6, Loss: 0.011317219585180283\n",
      "Epoch 6, Loss: 0.008461585268378258\n",
      "Epoch 6, Loss: 0.004857528489083052\n",
      "Epoch 6, Loss: 0.008514650166034698\n",
      "Epoch 6, Loss: 0.002691938541829586\n",
      "Epoch 6, Loss: 0.09518993645906448\n",
      "Epoch 6, Loss: 0.0006919694133102894\n",
      "Epoch 6, Loss: 0.003092545783147216\n",
      "Epoch 6, Loss: 0.005143577698618174\n",
      "Epoch 6, Loss: 0.08042243123054504\n",
      "Epoch 6, Loss: 0.001404734211973846\n",
      "Epoch 6, Loss: 0.4943431317806244\n",
      "Epoch 6, Loss: 0.00400201790034771\n",
      "Epoch 6, Loss: 0.04388105496764183\n",
      "Epoch 6, Loss: 0.021124688908457756\n",
      "Epoch 6, Loss: 0.020982105284929276\n",
      "Epoch 6, Loss: 0.02360854670405388\n",
      "Epoch 6, Loss: 0.025873377919197083\n",
      "Epoch 6, Loss: 0.09383439272642136\n",
      "Epoch 6, Loss: 0.04341807342910518\n",
      "Epoch 7, Loss: 0.053639255464076996\n",
      "Epoch 7, Loss: 0.07055208832025528\n",
      "Epoch 7, Loss: 0.023039698600769043\n",
      "Epoch 7, Loss: 0.027331383898854256\n",
      "Epoch 7, Loss: 0.012464887462556362\n",
      "Epoch 7, Loss: 0.0029372870922088623\n",
      "Epoch 7, Loss: 0.0007175045320764184\n",
      "Epoch 7, Loss: 0.2879304885864258\n",
      "Epoch 7, Loss: 0.0003713582700584084\n",
      "Epoch 7, Loss: 0.0012908512726426125\n",
      "Epoch 7, Loss: 0.0004790641542058438\n",
      "Epoch 7, Loss: 0.001309396349824965\n",
      "Epoch 7, Loss: 0.0011364702368155122\n",
      "Epoch 7, Loss: 0.16793178021907806\n",
      "Epoch 7, Loss: 0.00193112064152956\n",
      "Epoch 7, Loss: 0.0928012803196907\n",
      "Epoch 7, Loss: 0.0037975141312927008\n",
      "Epoch 7, Loss: 0.0038094553165137768\n",
      "Epoch 7, Loss: 0.002619165927171707\n",
      "Epoch 7, Loss: 0.008711635135114193\n",
      "Epoch 7, Loss: 0.17299239337444305\n",
      "Epoch 7, Loss: 0.07292213290929794\n",
      "Epoch 7, Loss: 0.07703056186437607\n",
      "Epoch 7, Loss: 0.016947148367762566\n",
      "Epoch 7, Loss: 0.020553655922412872\n",
      "Epoch 7, Loss: 0.03519916534423828\n",
      "Epoch 7, Loss: 0.018360162153840065\n",
      "Epoch 7, Loss: 0.042618829756975174\n",
      "Epoch 7, Loss: 0.013955132104456425\n",
      "Epoch 7, Loss: 0.034565482288599014\n",
      "Epoch 7, Loss: 0.030238419771194458\n",
      "Epoch 7, Loss: 0.1500978022813797\n",
      "Epoch 7, Loss: 0.1398521512746811\n",
      "Epoch 7, Loss: 0.008597617968916893\n",
      "Epoch 7, Loss: 0.011514491401612759\n",
      "Epoch 7, Loss: 0.01044924184679985\n",
      "Epoch 7, Loss: 0.0191221721470356\n",
      "Epoch 7, Loss: 0.013266407884657383\n",
      "Epoch 7, Loss: 0.008902360685169697\n",
      "Epoch 7, Loss: 0.002283766632899642\n",
      "Epoch 7, Loss: 0.0023410185240209103\n",
      "Epoch 7, Loss: 0.20611301064491272\n",
      "Epoch 7, Loss: 0.2987012565135956\n",
      "Epoch 7, Loss: 0.04448963701725006\n",
      "Epoch 7, Loss: 0.012410332448780537\n",
      "Epoch 7, Loss: 0.04951835637912154\n",
      "Epoch 8, Loss: 0.012694863602519035\n",
      "Epoch 8, Loss: 0.03605290874838829\n",
      "Epoch 8, Loss: 0.010372166521847248\n",
      "Epoch 8, Loss: 0.020548053085803986\n",
      "Epoch 8, Loss: 0.13101138174533844\n",
      "Epoch 8, Loss: 0.009508014656603336\n",
      "Epoch 8, Loss: 0.059587880969047546\n",
      "Epoch 8, Loss: 0.007959859445691109\n",
      "Epoch 8, Loss: 0.1563873142004013\n",
      "Epoch 8, Loss: 0.08835239708423615\n",
      "Epoch 8, Loss: 0.06195300817489624\n",
      "Epoch 8, Loss: 0.005917828064411879\n",
      "Epoch 8, Loss: 0.003950473852455616\n",
      "Epoch 8, Loss: 0.006659196689724922\n",
      "Epoch 8, Loss: 0.12202794849872589\n",
      "Epoch 8, Loss: 0.012967964634299278\n",
      "Epoch 8, Loss: 0.00534425675868988\n",
      "Epoch 8, Loss: 0.0048479847609996796\n",
      "Epoch 8, Loss: 0.07070799916982651\n",
      "Epoch 8, Loss: 0.007110919337719679\n",
      "Epoch 8, Loss: 0.009406768716871738\n",
      "Epoch 8, Loss: 0.003723037661984563\n",
      "Epoch 8, Loss: 0.10216359794139862\n",
      "Epoch 8, Loss: 0.009956602938473225\n",
      "Epoch 8, Loss: 0.005893359892070293\n",
      "Epoch 8, Loss: 0.005643182434141636\n",
      "Epoch 8, Loss: 0.0013718754053115845\n",
      "Epoch 8, Loss: 0.06323401629924774\n",
      "Epoch 8, Loss: 0.002312844153493643\n",
      "Epoch 8, Loss: 0.0033127034548670053\n",
      "Epoch 8, Loss: 0.07427693158388138\n",
      "Epoch 8, Loss: 0.003619955852627754\n",
      "Epoch 8, Loss: 0.008937781676650047\n",
      "Epoch 8, Loss: 0.011160197667777538\n",
      "Epoch 8, Loss: 0.018749697133898735\n",
      "Epoch 8, Loss: 0.01560303196310997\n",
      "Epoch 8, Loss: 0.025323182344436646\n",
      "Epoch 8, Loss: 0.08757378906011581\n",
      "Epoch 8, Loss: 0.013027803041040897\n",
      "Epoch 8, Loss: 0.009421776048839092\n",
      "Epoch 8, Loss: 0.008695508353412151\n",
      "Epoch 8, Loss: 0.1549263745546341\n",
      "Epoch 8, Loss: 0.02685185894370079\n",
      "Epoch 8, Loss: 0.004705940373241901\n",
      "Epoch 8, Loss: 0.08057507127523422\n",
      "Epoch 8, Loss: 0.03520954019493527\n",
      "Epoch 9, Loss: 0.004534074570983648\n",
      "Epoch 9, Loss: 0.05424999073147774\n",
      "Epoch 9, Loss: 0.006127507891505957\n",
      "Epoch 9, Loss: 0.006357819773256779\n",
      "Epoch 9, Loss: 0.11706379055976868\n",
      "Epoch 9, Loss: 0.004586109425872564\n",
      "Epoch 9, Loss: 0.008271519094705582\n",
      "Epoch 9, Loss: 0.016704261302947998\n",
      "Epoch 9, Loss: 0.019606230780482292\n",
      "Epoch 9, Loss: 0.011207474395632744\n",
      "Epoch 9, Loss: 0.04558712989091873\n",
      "Epoch 9, Loss: 0.004864376503974199\n",
      "Epoch 9, Loss: 0.002052308525890112\n",
      "Epoch 9, Loss: 0.043266404420137405\n",
      "Epoch 9, Loss: 0.001276528462767601\n",
      "Epoch 9, Loss: 0.03861081600189209\n",
      "Epoch 9, Loss: 0.004105215426534414\n",
      "Epoch 9, Loss: 0.18541818857192993\n",
      "Epoch 9, Loss: 0.0017783098155632615\n",
      "Epoch 9, Loss: 0.004309407435357571\n",
      "Epoch 9, Loss: 0.026604123413562775\n",
      "Epoch 9, Loss: 0.0072992523200809956\n",
      "Epoch 9, Loss: 0.08251512795686722\n",
      "Epoch 9, Loss: 0.005456485785543919\n",
      "Epoch 9, Loss: 0.005291877314448357\n",
      "Epoch 9, Loss: 0.11193883419036865\n",
      "Epoch 9, Loss: 0.10073159635066986\n",
      "Epoch 9, Loss: 0.008425463922321796\n",
      "Epoch 9, Loss: 0.013231873512268066\n",
      "Epoch 9, Loss: 0.00920609850436449\n",
      "Epoch 9, Loss: 0.02540319785475731\n",
      "Epoch 9, Loss: 0.06453392654657364\n",
      "Epoch 9, Loss: 0.005915770772844553\n",
      "Epoch 9, Loss: 0.15440680086612701\n",
      "Epoch 9, Loss: 0.01686963252723217\n",
      "Epoch 9, Loss: 0.027397019788622856\n",
      "Epoch 9, Loss: 0.004786442965269089\n",
      "Epoch 9, Loss: 0.03235893324017525\n",
      "Epoch 9, Loss: 0.08438345789909363\n",
      "Epoch 9, Loss: 0.0039304825477302074\n",
      "Epoch 9, Loss: 0.017176499590277672\n",
      "Epoch 9, Loss: 0.010091125965118408\n",
      "Epoch 9, Loss: 0.005437642335891724\n",
      "Epoch 9, Loss: 0.005540161859244108\n",
      "Epoch 9, Loss: 0.0018528996733948588\n",
      "Epoch 9, Loss: 0.03135027091743218\n",
      "Epoch 10, Loss: 0.03405602276325226\n",
      "Epoch 10, Loss: 0.0011980063281953335\n",
      "Epoch 10, Loss: 0.0037164040841162205\n",
      "Epoch 10, Loss: 0.0032895654439926147\n",
      "Epoch 10, Loss: 0.037611328065395355\n",
      "Epoch 10, Loss: 0.025382643565535545\n",
      "Epoch 10, Loss: 0.007405316922813654\n",
      "Epoch 10, Loss: 0.004345717374235392\n",
      "Epoch 10, Loss: 0.00035203108564019203\n",
      "Epoch 10, Loss: 0.004584095906466246\n",
      "Epoch 10, Loss: 0.003129655495285988\n",
      "Epoch 10, Loss: 0.0002309355913894251\n",
      "Epoch 10, Loss: 0.0003965742653235793\n",
      "Epoch 10, Loss: 7.719300629105419e-05\n",
      "Epoch 10, Loss: 0.0003244649851694703\n",
      "Epoch 10, Loss: 0.21965046226978302\n",
      "Epoch 10, Loss: 0.06011616066098213\n",
      "Epoch 10, Loss: 0.0017881735693663359\n",
      "Epoch 10, Loss: 0.010817386209964752\n",
      "Epoch 10, Loss: 0.0074446676298975945\n",
      "Epoch 10, Loss: 0.0042705172672867775\n",
      "Epoch 10, Loss: 0.009495772421360016\n",
      "Epoch 10, Loss: 0.06441392004489899\n",
      "Epoch 10, Loss: 0.17404012382030487\n",
      "Epoch 10, Loss: 0.06771870702505112\n",
      "Epoch 10, Loss: 0.04554170370101929\n",
      "Epoch 10, Loss: 0.04864376038312912\n",
      "Epoch 10, Loss: 0.03580719232559204\n",
      "Epoch 10, Loss: 0.07372338324785233\n",
      "Epoch 10, Loss: 0.03595251962542534\n",
      "Epoch 10, Loss: 0.004396619275212288\n",
      "Epoch 10, Loss: 0.004298343323171139\n",
      "Epoch 10, Loss: 0.004488285630941391\n",
      "Epoch 10, Loss: 0.0024601807817816734\n",
      "Epoch 10, Loss: 0.0013415967114269733\n",
      "Epoch 10, Loss: 0.0006575107108801603\n",
      "Epoch 10, Loss: 0.03862544149160385\n",
      "Epoch 10, Loss: 0.0010307967895641923\n",
      "Epoch 10, Loss: 0.001688338234089315\n",
      "Epoch 10, Loss: 0.0004437864408828318\n",
      "Epoch 10, Loss: 0.0007650728803128004\n",
      "Epoch 10, Loss: 0.2226450890302658\n",
      "Epoch 10, Loss: 0.1022472158074379\n",
      "Epoch 10, Loss: 0.0009225643007084727\n",
      "Epoch 10, Loss: 0.15374092757701874\n",
      "Epoch 10, Loss: 0.03389502609045141\n",
      "Accuracy on the test set: 98.88888888888889%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Elephant       0.00      0.00      0.00         4\n",
      "    Elephant       0.99      1.00      0.99       356\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.49      0.50      0.50       360\n",
      "weighted avg       0.98      0.99      0.98       360\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   4]\n",
      " [  0 356]]\n",
      "Average Accuracy: 98.88888888888887%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'D:/folder/SRIP_Task/one_vs_rest_dataset'  # Adjust this path\n",
    "n_folds = 3\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for fold in range(1, n_folds+1):\n",
    "    print(f'Starting fold {fold}')\n",
    "    train_loader, test_loader = get_data_loader(root_dir, fold, batch_size)\n",
    "    model = SimpleCNN()\n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, epochs)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Average Accuracy: {average_accuracy}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

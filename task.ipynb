{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is the task that was given as a pre requisite for the project 5 of SRIP 2024\n",
    "## Pratham Sharda(pratham.sharda@iitgn.ac.in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with resnet\n",
    "\n",
    "So here first i have divided the dataset for two classes(I have take it for crab and flamingo) and first have run it without crossvalidation due to time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "traindir = \"D:/SRIP/Binary_task/Training\"\n",
    "testdir = \"D:/SRIP/Binary_task/Validation\"\n",
    "\n",
    "#transformations\n",
    "train_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.ToTensor(),                                \n",
    "                                       torchvision.transforms.Normalize(\n",
    "                                           mean=[0.485, 0.456, 0.406],\n",
    "                                           std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "                                       ])\n",
    "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      torchvision.transforms.Normalize(\n",
    "                                          mean=[0.485, 0.456, 0.406],\n",
    "                                          std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "                                      ])\n",
    "\n",
    "#datasets\n",
    "train_data = datasets.ImageFolder(traindir,transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(testdir,transform=test_transforms)\n",
    "\n",
    "#dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_train_step(model, optimizer, loss_fn):\n",
    "  def train_step(x,y):\n",
    "    #make prediction\n",
    "    yhat = model(x)\n",
    "    #enter train mode\n",
    "    model.train()\n",
    "    #compute loss\n",
    "    loss = loss_fn(yhat,y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    #optimizer.cleargrads()\n",
    "\n",
    "    return loss\n",
    "  return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "#freeze all params\n",
    "for params in model.parameters():\n",
    "  params.requires_grad_ = False\n",
    "\n",
    "#add a new final layer\n",
    "nr_filters = model.fc.in_features  #number of input features of last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#loss\n",
    "loss_fn = BCEWithLogitsLoss() #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters()) \n",
    "\n",
    "#train step\n",
    "train_step = make_train_step(model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "n_epochs = 10\n",
    "early_stopping_tolerance = 3\n",
    "early_stopping_threshold = 0.03\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  epoch_loss = 0\n",
    "  for i ,data in tqdm(enumerate(trainloader), total = len(trainloader)): #iterate ove batches\n",
    "    x_batch , y_batch = data\n",
    "    x_batch = x_batch.to(device) #move to gpu\n",
    "    y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
    "    y_batch = y_batch.to(device) #move to gpu\n",
    "\n",
    "\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "    epoch_loss += loss/len(trainloader)\n",
    "    losses.append(loss)\n",
    "    \n",
    "  epoch_train_losses.append(epoch_loss)\n",
    "  print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
    "\n",
    "  #validation doesnt requires gradient\n",
    "  with torch.no_grad():\n",
    "    cum_loss = 0\n",
    "    for x_batch, y_batch in testloader:\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
    "      y_batch = y_batch.to(device)\n",
    "\n",
    "      #model to eval mode\n",
    "      model.eval()\n",
    "\n",
    "      yhat = model(x_batch)\n",
    "      val_loss = loss_fn(yhat,y_batch)\n",
    "      cum_loss += loss/len(testloader)\n",
    "      val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "    epoch_test_losses.append(cum_loss)\n",
    "    print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))  \n",
    "    \n",
    "    best_loss = min(epoch_test_losses)\n",
    "    \n",
    "    #save best model\n",
    "    if cum_loss <= best_loss:\n",
    "      best_model_wts = model.state_dict()\n",
    "    \n",
    "    #early stopping\n",
    "    early_stopping_counter = 0\n",
    "    if cum_loss > best_loss:\n",
    "      early_stopping_counter +=1\n",
    "\n",
    "    if (early_stopping_counter == early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
    "      print(\"/nTerminating: early stopping\")\n",
    "      break #terminate training\n",
    "    \n",
    "#load best model\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def inference(test_data):\n",
    "  idx = torch.randint(1, len(test_data), (1,))\n",
    "  sample = torch.unsqueeze(test_data[idx][0], dim=0).to(device)\n",
    "\n",
    "  if torch.sigmoid(model(sample)) < 0.5:\n",
    "    print(\"Prediction : Crab\")\n",
    "  else:\n",
    "    print(\"Prediction : Flamingo\")\n",
    "\n",
    "\n",
    "  plt.imshow(test_data[idx][0].permute(1, 2, 0))\n",
    "\n",
    "inference(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with cross validation(3 fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define directories (adjust paths as needed)\n",
    "traindir = \"D:/SRIP/Binary_task/Training\"  # Assuming this now includes both training and validation data\n",
    "\n",
    "# Transformations\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "dataset = datasets.ImageFolder(traindir, transform=transforms)\n",
    "\n",
    "# KFold configuration\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Loss function\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# Cross-validation\n",
    "best_loss_overall = np.inf\n",
    "best_model_wts_overall = None\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"Fold {fold+1}/{kfold.n_splits}\")\n",
    "\n",
    "    # Sampler for splitting data\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Data loaders for training and validation in this fold\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "    # Model setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 1)\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{n_epochs}, Fold {fold+1}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.unsqueeze(1).float()  # Ensure labels are float for BCELoss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader.sampler)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.unsqueeze(1).float()\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(testloader.sampler)\n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}: Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if best_loss < best_loss_overall:\n",
    "        best_loss_overall = best_loss\n",
    "        best_model_wts_overall = copy.deepcopy(best_model_wts)\n",
    "\n",
    "# Load the best model weights from cross-validation\n",
    "model.load_state_dict(best_model_wts_overall)\n",
    "\n",
    "# You can now proceed with testing or inference using this model\n",
    "# For example, here's a simple function to do inference on a single image from the dataset\n",
    "def inference(dataset, model, device):\n",
    "    model.eval()\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    img, _ = dataset[idx]\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension and transfer to device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "\n",
    "    plt.imshow(dataset[idx][0].permute(1, 2, 0))\n",
    "    plt.title(f\"Prediction: {'Flamingo' if prediction >= 0.5 else 'Crab'}\")\n",
    "    plt.show()\n",
    "\n",
    "# Example inference\n",
    "inference(dataset, model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So now i have created the dataset for 5 class and one vs all classification\n",
    "\n",
    "So I have take here for one vs rest classification the example of elephant that is is elephant or is not elephant\n",
    "For 5 class I have converted the dataset into 5 groups where each group is divided based on some particular characteristics(mammals,reptiles,birds,insects and aquatic animals).This has been explicitly mentioned in the notebook below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_conv_layers(model, input_image):\n",
    "#     # Assuming input_image is a torch.Tensor of shape (C, H, W) and normalized\n",
    "#     activation = {}\n",
    "#     def get_activation(name):\n",
    "#         def hook(model, input, output):\n",
    "#             activation[name] = output.detach()\n",
    "#         return hook\n",
    "    \n",
    "#     # Register hooks\n",
    "#     model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "#     model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "#     model.conv3.register_forward_hook(get_activation('conv3'))\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(input_image.unsqueeze(0)) # Add batch dimension\n",
    "    \n",
    "#     # Plotting\n",
    "#     for name, act in activation.items():\n",
    "#         num_feature_maps = act.size(1)\n",
    "#         # Plot feature maps\n",
    "#         # You can use matplotlib to create subplots and plot each feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating one vs rest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source directory where the current dataset is stored\n",
    "source_directory = 'D:/SRIP/archive/animals/animals'\n",
    "\n",
    "# Define the destination directory where the new folder structure will be created\n",
    "destination_directory = 'D:/SRIP/one_vs_rest_dataset'\n",
    "\n",
    "# Define the names of the new subdirectories\n",
    "elephant_dir = os.path.join(destination_directory, 'elephant')\n",
    "other_animals_dir = os.path.join(destination_directory, 'other_animals')\n",
    "\n",
    "# Create the destination directory and subdirectories if they don't already exist\n",
    "os.makedirs(elephant_dir, exist_ok=True)\n",
    "os.makedirs(other_animals_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each folder in the source directory\n",
    "for folder_name in os.listdir(source_directory):\n",
    "    # Define the path to the current folder\n",
    "    current_folder_path = os.path.join(source_directory, folder_name)\n",
    "    \n",
    "    # Check if the current folder is indeed a directory\n",
    "    if os.path.isdir(current_folder_path):\n",
    "        # Determine the destination directory based on whether the folder is 'elephant' or not\n",
    "        if folder_name.lower() == 'elephant':\n",
    "            dest_dir = elephant_dir\n",
    "        else:\n",
    "            dest_dir = other_animals_dir\n",
    "        \n",
    "        # Loop through each file in the current folder\n",
    "        for filename in os.listdir(current_folder_path):\n",
    "            # Define the source and destination file paths\n",
    "            source_file_path = os.path.join(current_folder_path, filename)\n",
    "            destination_file_path = os.path.join(dest_dir, filename)\n",
    "            \n",
    "            # Copy the file from the source to the destination\n",
    "            shutil.copy(source_file_path, destination_file_path)\n",
    "\n",
    "print(\"Dataset reorganization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 fold dataset creation for elephant vs rest animals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory containing the elephant and other_animals folders\n",
    "dataset_directory = 'D:/SRIP/one_vs_rest_dataset'\n",
    "\n",
    "# Define the main categories\n",
    "categories = ['elephant', 'other_animals']\n",
    "\n",
    "# Initialize KFold with 3 splits\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Process each category separately to split them into folds\n",
    "for category in categories:\n",
    "    # Path to the specific category directory\n",
    "    category_path = os.path.join(dataset_directory, category)\n",
    "    \n",
    "    # List all files in the category directory\n",
    "    files = np.array(os.listdir(category_path))\n",
    "    \n",
    "    # Apply KFold splitting\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(files)):\n",
    "        # Paths for train and validation directories for this fold\n",
    "        fold_dir = os.path.join(dataset_directory, f'fold_{fold+1}', category)\n",
    "        \n",
    "        # Create the fold directory if it doesn't exist\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        # Validation files for this fold\n",
    "        val_files = files[val_idx]\n",
    "        \n",
    "        # Copy validation files to the fold directory\n",
    "        for file in val_files:\n",
    "            src_file_path = os.path.join(category_path, file)\n",
    "            dst_file_path = os.path.join(fold_dir, file)\n",
    "            shutil.copy(src_file_path, dst_file_path)\n",
    "\n",
    "print(\"3-Fold dataset split complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Class dataset creation from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define your source and destination folders\n",
    "src_folder = 'D:/SRIP/archive/animals/animals'\n",
    "dest_folder = 'D:/SRIP/5_class_dataset'\n",
    "\n",
    "# Mapping of animals to their classes\n",
    "class_mapping = {\n",
    "    'Mammals': [\"Antelope\", \"Bear\", \"Bison\", \"Cat\", \"Chimpanzee\", \"Cow\", \"Coyote\", \"Deer\", \"Dog\", \"Dolphin\", \"Elephant\", \"Fox\", \"Gorilla\", \"Kangaroo\", \"Koala\", \"Leopard\", \"Lion\", \"Otter\", \"Panda\", \"Porcupine\", \"Raccoon\", \"Reindeer\", \"Rhinoceros\", \"Tiger\", \"Whale\", \"Wolf\", \"Zebra\"],\n",
    "    'Birds': [\"Bat\", \"Eagle\", \"Flamingo\", \"Hummingbird\", \"Owl\", \"Parrot\", \"Pelecaniformes\", \"Penguin\", \"Pigeon\", \"Sparrow\", \"Turkey\", \"Woodpecker\"],\n",
    "    'Aquatic and Amphibious Animals': [\"Crab\", \"Dolphin\", \"Goldfish\", \"Jellyfish\", \"Lobster\", \"Octopus\", \"Oyster\", \"Seahorse\", \"Seal\", \"Shark\", \"Starfish\"],\n",
    "    'Insects and Arthropods': [\"Bee\", \"Beetle\", \"Butterfly\", \"Caterpillar\", \"Cockroach\", \"Dragonfly\", \"Fly\", \"Grasshopper\", \"Ladybugs\", \"Mosquito\", \"Moth\"],\n",
    "    'Reptiles and Others': [\"Lizard\", \"Snake\", \"Turtle\"]  # Assuming 'Crocodile' and 'Tortoise' aren't in your list but could be added if they were.\n",
    "}\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(dest_folder):\n",
    "    os.makedirs(dest_folder)\n",
    "\n",
    "# Create class folders and move files\n",
    "for class_name, animals in class_mapping.items():\n",
    "    class_folder = os.path.join(dest_folder, class_name)\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "    \n",
    "    for animal in animals:\n",
    "        animal_folder = os.path.join(src_folder, animal)\n",
    "        if os.path.exists(animal_folder):\n",
    "            for filename in os.listdir(animal_folder):\n",
    "                src_file = os.path.join(animal_folder, filename)\n",
    "                dest_file = os.path.join(class_folder, filename)\n",
    "                # To avoid overwriting files with the same name from different folders, you could add a check here\n",
    "                shutil.move(src_file, dest_file)\n",
    "\n",
    "print(\"Images have been successfully reorganized into class-based folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Craeting 3 fold dataset from the 5 class dataset so that we can have 3 folds for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory containing the elephant and other_animals folders\n",
    "dataset_directory = 'D:/SRIP/5_class_dataset'\n",
    "\n",
    "# Define the main categories\n",
    "categories = ['Mammals', 'Birds', 'Aquatic and Amphibious Animals', 'Insects and Arthropods', 'Reptiles and Others']\n",
    "\n",
    "# Initialize KFold with 3 splits\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Process each category separately to split them into folds\n",
    "for category in categories:\n",
    "    # Path to the specific category directory\n",
    "    category_path = os.path.join(dataset_directory, category)\n",
    "    \n",
    "    # List all files in the category directory\n",
    "    files = np.array(os.listdir(category_path))\n",
    "    \n",
    "    # Apply KFold splitting\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(files)):\n",
    "        # Paths for train and validation directories for this fold\n",
    "        fold_dir = os.path.join(dataset_directory, f'fold_{fold+1}', category)\n",
    "        \n",
    "        # Create the fold directory if it doesn't exist\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        # Validation files for this fold\n",
    "        val_files = files[val_idx]\n",
    "        \n",
    "        # Copy validation files to the fold directory\n",
    "        for file in val_files:\n",
    "            src_file_path = os.path.join(category_path, file)\n",
    "            dst_file_path = os.path.join(fold_dir, file)\n",
    "            shutil.copy(src_file_path, dst_file_path)\n",
    "\n",
    "print(\"3-Fold dataset split complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=2): # Default binary for one-vs-rest\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(128 * 28 * 28, 512) # Adjust the size according to your input\n",
    "#         self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 28 * 28) # Adjust the size according to your input\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL FOR 5 CLASS \n",
    "\n",
    "Now that we have created the datset for the model.\n",
    "\n",
    "\n",
    "This custom CNN is designed for image classification tasks. It consists of two convolutional layers followed by max pooling layers for feature extraction and spatial downsampling. The convolutional layers use ReLU activation functions. The output from the convolutional layers is flattened and passed through two fully connected layers with ReLU activation and at last having softmax activation function as it is multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=2): # Default binary for one-vs-rest\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(128 * 28 * 28, 512) # Adjust the size according to your input\n",
    "#         self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 28 * 28) # Adjust the size according to your input\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 5)  # Assuming 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i am dividing each fold into training and test dataset and then fitting on the model and calculating the metrics to check its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_dir, fold):\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_datasets = []\n",
    "    test_dataset = None\n",
    "    for i in range(1, 4):  # For fold 1, fold 2, fold 3\n",
    "        fold_path = f'{data_dir}/fold_{i}'\n",
    "        if i == fold:\n",
    "            test_dataset = datasets.ImageFolder(root=fold_path, transform=image_transform)\n",
    "        else:\n",
    "            train_datasets.append(datasets.ImageFolder(root=fold_path, transform=image_transform))\n",
    "\n",
    "    train_dataset = ConcatDataset(train_datasets)\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold 1\n",
      "Epoch 1, Loss: 1.6240241527557373\n",
      "Epoch 1, Loss: 8.057486534118652\n",
      "Epoch 1, Loss: 5.731037139892578\n",
      "Epoch 1, Loss: 6.722264289855957\n",
      "Epoch 1, Loss: 3.2569351196289062\n",
      "Epoch 1, Loss: 1.7579600811004639\n",
      "Epoch 1, Loss: 1.6667135953903198\n",
      "Epoch 1, Loss: 1.6080600023269653\n",
      "Epoch 1, Loss: 1.5718687772750854\n",
      "Epoch 1, Loss: 1.5815346240997314\n",
      "Epoch 1, Loss: 1.6057149171829224\n",
      "Epoch 1, Loss: 1.6013282537460327\n",
      "Epoch 1, Loss: 1.5987441539764404\n",
      "Epoch 1, Loss: 1.534095287322998\n",
      "Epoch 1, Loss: 1.5452967882156372\n",
      "Epoch 1, Loss: 1.5654211044311523\n",
      "Epoch 1, Loss: 1.5082027912139893\n",
      "Epoch 1, Loss: 1.6085922718048096\n",
      "Epoch 1, Loss: 1.6096221208572388\n",
      "Epoch 1, Loss: 1.4163612127304077\n",
      "Epoch 1, Loss: 1.7120956182479858\n",
      "Epoch 1, Loss: 1.5537605285644531\n",
      "Epoch 1, Loss: 1.4108872413635254\n",
      "Epoch 1, Loss: 1.3780244588851929\n",
      "Epoch 1, Loss: 1.5497326850891113\n",
      "Epoch 1, Loss: 1.4746969938278198\n",
      "Epoch 1, Loss: 1.4788645505905151\n",
      "Epoch 1, Loss: 1.4680649042129517\n",
      "Epoch 1, Loss: 1.4550009965896606\n",
      "Epoch 1, Loss: 1.6040583848953247\n",
      "Epoch 1, Loss: 1.6104482412338257\n",
      "Epoch 1, Loss: 1.5714682340621948\n",
      "Epoch 1, Loss: 1.4571224451065063\n",
      "Epoch 1, Loss: 1.436072826385498\n",
      "Epoch 1, Loss: 1.4428414106369019\n",
      "Epoch 1, Loss: 1.6701737642288208\n",
      "Epoch 1, Loss: 1.4709033966064453\n",
      "Epoch 1, Loss: 1.4963423013687134\n",
      "Epoch 1, Loss: 1.5194658041000366\n",
      "Epoch 1, Loss: 1.5645194053649902\n",
      "Epoch 1, Loss: 1.3899410963058472\n",
      "Epoch 1, Loss: 1.4016788005828857\n",
      "Epoch 1, Loss: 1.4921901226043701\n",
      "Epoch 1, Loss: 1.3249258995056152\n",
      "Epoch 1, Loss: 1.412618637084961\n",
      "Epoch 1, Loss: 1.4130126237869263\n",
      "Epoch 1, Loss: 1.3508256673812866\n",
      "Epoch 1, Loss: 1.0718257427215576\n",
      "Epoch 1, Loss: 1.5961525440216064\n",
      "Epoch 1, Loss: 1.2638630867004395\n",
      "Epoch 1, Loss: 1.4056248664855957\n",
      "Epoch 1, Loss: 1.6048558950424194\n",
      "Epoch 1, Loss: 1.7717889547348022\n",
      "Epoch 1, Loss: 1.394486427307129\n",
      "Epoch 1, Loss: 1.4299112558364868\n",
      "Epoch 1, Loss: 1.4355195760726929\n",
      "Epoch 1, Loss: 1.4312782287597656\n",
      "Epoch 1, Loss: 1.4160315990447998\n",
      "Epoch 1, Loss: 1.4276471138000488\n",
      "Epoch 1, Loss: 1.278340458869934\n",
      "Epoch 1, Loss: 1.3320845365524292\n",
      "Epoch 1, Loss: 1.8531712293624878\n",
      "Epoch 1, Loss: 1.4578391313552856\n",
      "Epoch 1, Loss: 1.6920462846755981\n",
      "Epoch 1, Loss: 1.427046298980713\n",
      "Epoch 1, Loss: 1.4388107061386108\n",
      "Epoch 1, Loss: 1.4160346984863281\n",
      "Epoch 1, Loss: 1.4785844087600708\n",
      "Epoch 1, Loss: 1.3809417486190796\n",
      "Epoch 1, Loss: 1.4792019128799438\n",
      "Epoch 1, Loss: 1.526356816291809\n",
      "Epoch 1, Loss: 1.555633544921875\n",
      "Epoch 1, Loss: 1.4261844158172607\n",
      "Epoch 1, Loss: 1.4321725368499756\n",
      "Epoch 1, Loss: 1.7803072929382324\n",
      "Epoch 1, Loss: 1.4247151613235474\n",
      "Epoch 1, Loss: 1.5336873531341553\n",
      "Epoch 1, Loss: 1.4421985149383545\n",
      "Epoch 1, Loss: 1.2741450071334839\n",
      "Epoch 2, Loss: 1.3291893005371094\n",
      "Epoch 2, Loss: 1.4833039045333862\n",
      "Epoch 2, Loss: 1.3471332788467407\n",
      "Epoch 2, Loss: 1.5684778690338135\n",
      "Epoch 2, Loss: 1.4450123310089111\n",
      "Epoch 2, Loss: 1.4669404029846191\n",
      "Epoch 2, Loss: 1.3143200874328613\n",
      "Epoch 2, Loss: 1.3685424327850342\n",
      "Epoch 2, Loss: 1.2475183010101318\n",
      "Epoch 2, Loss: 1.4427889585494995\n",
      "Epoch 2, Loss: 1.4309252500534058\n",
      "Epoch 2, Loss: 1.4418611526489258\n",
      "Epoch 2, Loss: 1.4368171691894531\n",
      "Epoch 2, Loss: 1.1184364557266235\n",
      "Epoch 2, Loss: 1.199064016342163\n",
      "Epoch 2, Loss: 1.4031468629837036\n",
      "Epoch 2, Loss: 1.2666257619857788\n",
      "Epoch 2, Loss: 1.3768938779830933\n",
      "Epoch 2, Loss: 1.2812894582748413\n",
      "Epoch 2, Loss: 1.4133483171463013\n",
      "Epoch 2, Loss: 1.3670728206634521\n",
      "Epoch 2, Loss: 1.573695182800293\n",
      "Epoch 2, Loss: 1.1118593215942383\n",
      "Epoch 2, Loss: 1.3340321779251099\n",
      "Epoch 2, Loss: 1.2811367511749268\n",
      "Epoch 2, Loss: 1.388692021369934\n",
      "Epoch 2, Loss: 1.21312415599823\n",
      "Epoch 2, Loss: 1.3710777759552002\n",
      "Epoch 2, Loss: 1.326504111289978\n",
      "Epoch 2, Loss: 1.455168604850769\n",
      "Epoch 2, Loss: 1.4196031093597412\n",
      "Epoch 2, Loss: 1.1791852712631226\n",
      "Epoch 2, Loss: 1.3876322507858276\n",
      "Epoch 2, Loss: 1.281310796737671\n",
      "Epoch 2, Loss: 1.3234093189239502\n",
      "Epoch 2, Loss: 1.1332062482833862\n",
      "Epoch 2, Loss: 1.2785543203353882\n",
      "Epoch 2, Loss: 1.4572280645370483\n",
      "Epoch 2, Loss: 1.423185110092163\n",
      "Epoch 2, Loss: 1.4828579425811768\n",
      "Epoch 2, Loss: 1.3518590927124023\n",
      "Epoch 2, Loss: 1.4099842309951782\n",
      "Epoch 2, Loss: 1.3882548809051514\n",
      "Epoch 2, Loss: 1.388796091079712\n",
      "Epoch 2, Loss: 1.3813730478286743\n",
      "Epoch 2, Loss: 1.3463712930679321\n",
      "Epoch 2, Loss: 1.4148833751678467\n",
      "Epoch 2, Loss: 1.3451608419418335\n",
      "Epoch 2, Loss: 1.3981413841247559\n",
      "Epoch 2, Loss: 1.2872192859649658\n",
      "Epoch 2, Loss: 1.3517037630081177\n",
      "Epoch 2, Loss: 1.4604116678237915\n",
      "Epoch 2, Loss: 1.3916113376617432\n",
      "Epoch 2, Loss: 1.3976776599884033\n",
      "Epoch 2, Loss: 1.1013360023498535\n",
      "Epoch 2, Loss: 1.3315845727920532\n",
      "Epoch 2, Loss: 1.1229718923568726\n",
      "Epoch 2, Loss: 1.379517912864685\n",
      "Epoch 2, Loss: 1.3549351692199707\n",
      "Epoch 2, Loss: 1.6793019771575928\n",
      "Epoch 2, Loss: 1.2888962030410767\n",
      "Epoch 2, Loss: 1.3929784297943115\n",
      "Epoch 2, Loss: 1.2913494110107422\n",
      "Epoch 2, Loss: 1.2310906648635864\n",
      "Epoch 2, Loss: 1.2930772304534912\n",
      "Epoch 2, Loss: 1.2780592441558838\n",
      "Epoch 2, Loss: 1.3388786315917969\n",
      "Epoch 2, Loss: 1.3128443956375122\n",
      "Epoch 2, Loss: 1.2350177764892578\n",
      "Epoch 2, Loss: 1.5333867073059082\n",
      "Epoch 2, Loss: 1.1677136421203613\n",
      "Epoch 2, Loss: 1.3557056188583374\n",
      "Epoch 2, Loss: 1.4778571128845215\n",
      "Epoch 2, Loss: 1.109857201576233\n",
      "Epoch 2, Loss: 1.5067803859710693\n",
      "Epoch 2, Loss: 1.2364593744277954\n",
      "Epoch 2, Loss: 1.4187958240509033\n",
      "Epoch 2, Loss: 1.2584561109542847\n",
      "Epoch 2, Loss: 1.270024299621582\n",
      "Epoch 3, Loss: 1.1717860698699951\n",
      "Epoch 3, Loss: 1.3193095922470093\n",
      "Epoch 3, Loss: 1.2479853630065918\n",
      "Epoch 3, Loss: 1.362898349761963\n",
      "Epoch 3, Loss: 1.2990354299545288\n",
      "Epoch 3, Loss: 1.3829327821731567\n",
      "Epoch 3, Loss: 1.214083194732666\n",
      "Epoch 3, Loss: 1.2467726469039917\n",
      "Epoch 3, Loss: 1.3103474378585815\n",
      "Epoch 3, Loss: 1.061265468597412\n",
      "Epoch 3, Loss: 1.1830717325210571\n",
      "Epoch 3, Loss: 1.323134183883667\n",
      "Epoch 3, Loss: 1.1450384855270386\n",
      "Epoch 3, Loss: 1.2465168237686157\n",
      "Epoch 3, Loss: 1.0434215068817139\n",
      "Epoch 3, Loss: 1.1460895538330078\n",
      "Epoch 3, Loss: 1.195465087890625\n",
      "Epoch 3, Loss: 1.2885890007019043\n",
      "Epoch 3, Loss: 1.3802752494812012\n",
      "Epoch 3, Loss: 1.2029176950454712\n",
      "Epoch 3, Loss: 1.3919833898544312\n",
      "Epoch 3, Loss: 1.3202673196792603\n",
      "Epoch 3, Loss: 1.2426855564117432\n",
      "Epoch 3, Loss: 1.1610512733459473\n",
      "Epoch 3, Loss: 1.2304396629333496\n",
      "Epoch 3, Loss: 1.2359496355056763\n",
      "Epoch 3, Loss: 1.3410147428512573\n",
      "Epoch 3, Loss: 1.4012210369110107\n",
      "Epoch 3, Loss: 1.3182814121246338\n",
      "Epoch 3, Loss: 1.2687385082244873\n",
      "Epoch 3, Loss: 1.4939160346984863\n",
      "Epoch 3, Loss: 1.2338601350784302\n",
      "Epoch 3, Loss: 1.2667769193649292\n",
      "Epoch 3, Loss: 1.2209432125091553\n",
      "Epoch 3, Loss: 1.286695957183838\n",
      "Epoch 3, Loss: 1.1268359422683716\n",
      "Epoch 3, Loss: 1.2982559204101562\n",
      "Epoch 3, Loss: 1.388758897781372\n",
      "Epoch 3, Loss: 1.161296010017395\n",
      "Epoch 3, Loss: 1.2105404138565063\n",
      "Epoch 3, Loss: 1.2048230171203613\n",
      "Epoch 3, Loss: 1.4120351076126099\n",
      "Epoch 3, Loss: 0.975260317325592\n",
      "Epoch 3, Loss: 1.2419980764389038\n",
      "Epoch 3, Loss: 1.108077883720398\n",
      "Epoch 3, Loss: 1.5796617269515991\n",
      "Epoch 3, Loss: 1.3752988576889038\n",
      "Epoch 3, Loss: 1.1965553760528564\n",
      "Epoch 3, Loss: 1.16721773147583\n",
      "Epoch 3, Loss: 1.2528668642044067\n",
      "Epoch 3, Loss: 1.2908942699432373\n",
      "Epoch 3, Loss: 1.1432663202285767\n",
      "Epoch 3, Loss: 1.142764925956726\n",
      "Epoch 3, Loss: 1.1007167100906372\n",
      "Epoch 3, Loss: 0.9662047028541565\n",
      "Epoch 3, Loss: 1.1684167385101318\n",
      "Epoch 3, Loss: 1.1997824907302856\n",
      "Epoch 3, Loss: 1.1626574993133545\n",
      "Epoch 3, Loss: 1.020824909210205\n",
      "Epoch 3, Loss: 1.1908775568008423\n",
      "Epoch 3, Loss: 0.9010935425758362\n",
      "Epoch 3, Loss: 1.14008367061615\n",
      "Epoch 3, Loss: 1.1888922452926636\n",
      "Epoch 3, Loss: 0.99024897813797\n",
      "Epoch 3, Loss: 0.8202906250953674\n",
      "Epoch 3, Loss: 1.0541272163391113\n",
      "Epoch 3, Loss: 1.1401257514953613\n",
      "Epoch 3, Loss: 1.299042820930481\n",
      "Epoch 3, Loss: 1.1736187934875488\n",
      "Epoch 3, Loss: 1.1985092163085938\n",
      "Epoch 3, Loss: 0.9561923146247864\n",
      "Epoch 3, Loss: 1.4288852214813232\n",
      "Epoch 3, Loss: 1.1217153072357178\n",
      "Epoch 3, Loss: 1.2095564603805542\n",
      "Epoch 3, Loss: 1.2333531379699707\n",
      "Epoch 3, Loss: 1.0397529602050781\n",
      "Epoch 3, Loss: 0.9717005491256714\n",
      "Epoch 3, Loss: 1.096799373626709\n",
      "Epoch 3, Loss: 1.3616336584091187\n",
      "Epoch 4, Loss: 0.8538186550140381\n",
      "Epoch 4, Loss: 1.0839555263519287\n",
      "Epoch 4, Loss: 1.0632895231246948\n",
      "Epoch 4, Loss: 0.8634760975837708\n",
      "Epoch 4, Loss: 0.9534222483634949\n",
      "Epoch 4, Loss: 1.016129732131958\n",
      "Epoch 4, Loss: 1.0125765800476074\n",
      "Epoch 4, Loss: 0.9448549151420593\n",
      "Epoch 4, Loss: 1.1441962718963623\n",
      "Epoch 4, Loss: 0.9697213172912598\n",
      "Epoch 4, Loss: 1.282189130783081\n",
      "Epoch 4, Loss: 0.9551731944084167\n",
      "Epoch 4, Loss: 1.3080525398254395\n",
      "Epoch 4, Loss: 1.1775561571121216\n",
      "Epoch 4, Loss: 0.8816012740135193\n",
      "Epoch 4, Loss: 1.3759132623672485\n",
      "Epoch 4, Loss: 1.135665774345398\n",
      "Epoch 4, Loss: 1.0733684301376343\n",
      "Epoch 4, Loss: 0.9424538016319275\n",
      "Epoch 4, Loss: 0.9452652335166931\n",
      "Epoch 4, Loss: 0.9574393630027771\n",
      "Epoch 4, Loss: 0.7107642889022827\n",
      "Epoch 4, Loss: 0.936345100402832\n",
      "Epoch 4, Loss: 1.0568890571594238\n",
      "Epoch 4, Loss: 1.369527816772461\n",
      "Epoch 4, Loss: 1.3376513719558716\n",
      "Epoch 4, Loss: 0.916411817073822\n",
      "Epoch 4, Loss: 1.0385900735855103\n",
      "Epoch 4, Loss: 0.9578046202659607\n",
      "Epoch 4, Loss: 0.9612990617752075\n",
      "Epoch 4, Loss: 1.1139744520187378\n",
      "Epoch 4, Loss: 0.9853806495666504\n",
      "Epoch 4, Loss: 1.286267876625061\n",
      "Epoch 4, Loss: 1.1562083959579468\n",
      "Epoch 4, Loss: 1.073987364768982\n",
      "Epoch 4, Loss: 1.3131535053253174\n",
      "Epoch 4, Loss: 1.085017204284668\n",
      "Epoch 4, Loss: 0.9788885712623596\n",
      "Epoch 4, Loss: 0.7401245832443237\n",
      "Epoch 4, Loss: 1.032013177871704\n",
      "Epoch 4, Loss: 0.922243595123291\n",
      "Epoch 4, Loss: 1.0363655090332031\n",
      "Epoch 4, Loss: 0.6229676604270935\n",
      "Epoch 4, Loss: 1.22278892993927\n",
      "Epoch 4, Loss: 0.9124708771705627\n",
      "Epoch 4, Loss: 1.1663919687271118\n",
      "Epoch 4, Loss: 0.798322856426239\n",
      "Epoch 4, Loss: 1.005139946937561\n",
      "Epoch 4, Loss: 0.7694981694221497\n",
      "Epoch 4, Loss: 0.7412819862365723\n",
      "Epoch 4, Loss: 0.8697808980941772\n",
      "Epoch 4, Loss: 1.0069061517715454\n",
      "Epoch 4, Loss: 0.8907718658447266\n",
      "Epoch 4, Loss: 1.0800362825393677\n",
      "Epoch 4, Loss: 0.8479979634284973\n",
      "Epoch 4, Loss: 0.9398061633110046\n",
      "Epoch 4, Loss: 1.2237348556518555\n",
      "Epoch 4, Loss: 0.9561206102371216\n",
      "Epoch 4, Loss: 0.8147377371788025\n",
      "Epoch 4, Loss: 1.0804111957550049\n",
      "Epoch 4, Loss: 1.3017265796661377\n",
      "Epoch 4, Loss: 0.9891365170478821\n",
      "Epoch 4, Loss: 1.1478404998779297\n",
      "Epoch 4, Loss: 1.0052928924560547\n",
      "Epoch 4, Loss: 0.833452045917511\n",
      "Epoch 4, Loss: 1.0164096355438232\n",
      "Epoch 4, Loss: 1.198435664176941\n",
      "Epoch 4, Loss: 0.9603193998336792\n",
      "Epoch 4, Loss: 1.0328121185302734\n",
      "Epoch 4, Loss: 0.7134313583374023\n",
      "Epoch 4, Loss: 1.0771862268447876\n",
      "Epoch 4, Loss: 0.7193878889083862\n",
      "Epoch 4, Loss: 1.3218938112258911\n",
      "Epoch 4, Loss: 1.2183818817138672\n",
      "Epoch 4, Loss: 0.9149703979492188\n",
      "Epoch 4, Loss: 1.1958494186401367\n",
      "Epoch 4, Loss: 0.7770869135856628\n",
      "Epoch 4, Loss: 0.8104714155197144\n",
      "Epoch 4, Loss: 0.8356021046638489\n",
      "Epoch 5, Loss: 0.5419989824295044\n",
      "Epoch 5, Loss: 0.7992087006568909\n",
      "Epoch 5, Loss: 0.789540708065033\n",
      "Epoch 5, Loss: 0.6478674411773682\n",
      "Epoch 5, Loss: 0.8495123386383057\n",
      "Epoch 5, Loss: 0.537293553352356\n",
      "Epoch 5, Loss: 0.5884298086166382\n",
      "Epoch 5, Loss: 0.7781170010566711\n",
      "Epoch 5, Loss: 0.853702962398529\n",
      "Epoch 5, Loss: 0.7128536105155945\n",
      "Epoch 5, Loss: 0.7873838543891907\n",
      "Epoch 5, Loss: 0.7127016186714172\n",
      "Epoch 5, Loss: 0.5450220704078674\n",
      "Epoch 5, Loss: 0.9694046974182129\n",
      "Epoch 5, Loss: 0.5828071236610413\n",
      "Epoch 5, Loss: 0.4684695303440094\n",
      "Epoch 5, Loss: 0.5491032600402832\n",
      "Epoch 5, Loss: 0.6304923892021179\n",
      "Epoch 5, Loss: 0.5498647689819336\n",
      "Epoch 5, Loss: 0.6310423016548157\n",
      "Epoch 5, Loss: 0.736870527267456\n",
      "Epoch 5, Loss: 0.9649509191513062\n",
      "Epoch 5, Loss: 0.6309397220611572\n",
      "Epoch 5, Loss: 0.7631962299346924\n",
      "Epoch 5, Loss: 0.5584885478019714\n",
      "Epoch 5, Loss: 0.7827015519142151\n",
      "Epoch 5, Loss: 0.7785866260528564\n",
      "Epoch 5, Loss: 0.7713499069213867\n",
      "Epoch 5, Loss: 0.49799275398254395\n",
      "Epoch 5, Loss: 0.6650853753089905\n",
      "Epoch 5, Loss: 0.4158715307712555\n",
      "Epoch 5, Loss: 0.8692514896392822\n",
      "Epoch 5, Loss: 0.5118114948272705\n",
      "Epoch 5, Loss: 0.7559861540794373\n",
      "Epoch 5, Loss: 0.7885085940361023\n",
      "Epoch 5, Loss: 0.49768704175949097\n",
      "Epoch 5, Loss: 0.45387330651283264\n",
      "Epoch 5, Loss: 0.8698568940162659\n",
      "Epoch 5, Loss: 0.6316021680831909\n",
      "Epoch 5, Loss: 0.7037755846977234\n",
      "Epoch 5, Loss: 0.4727024435997009\n",
      "Epoch 5, Loss: 0.6550284028053284\n",
      "Epoch 5, Loss: 1.0192674398422241\n",
      "Epoch 5, Loss: 0.7288971543312073\n",
      "Epoch 5, Loss: 0.7287198901176453\n",
      "Epoch 5, Loss: 0.5129156112670898\n",
      "Epoch 5, Loss: 0.9296771287918091\n",
      "Epoch 5, Loss: 0.6747617721557617\n",
      "Epoch 5, Loss: 0.9290682077407837\n",
      "Epoch 5, Loss: 0.6929042935371399\n",
      "Epoch 5, Loss: 0.9084909558296204\n",
      "Epoch 5, Loss: 1.0073909759521484\n",
      "Epoch 5, Loss: 0.974628746509552\n",
      "Epoch 5, Loss: 0.7446975708007812\n",
      "Epoch 5, Loss: 0.8836939334869385\n",
      "Epoch 5, Loss: 0.9243720769882202\n",
      "Epoch 5, Loss: 0.7734454274177551\n",
      "Epoch 5, Loss: 0.6543999910354614\n",
      "Epoch 5, Loss: 0.7384628653526306\n",
      "Epoch 5, Loss: 0.737209141254425\n",
      "Epoch 5, Loss: 0.5838973522186279\n",
      "Epoch 5, Loss: 1.0077530145645142\n",
      "Epoch 5, Loss: 0.8381844758987427\n",
      "Epoch 5, Loss: 1.3337185382843018\n",
      "Epoch 5, Loss: 0.47982922196388245\n",
      "Epoch 5, Loss: 0.649187445640564\n",
      "Epoch 5, Loss: 0.5436604619026184\n",
      "Epoch 5, Loss: 0.7083343267440796\n",
      "Epoch 5, Loss: 0.8175519704818726\n",
      "Epoch 5, Loss: 0.6847981810569763\n",
      "Epoch 5, Loss: 0.8431162238121033\n",
      "Epoch 5, Loss: 0.6624211668968201\n",
      "Epoch 5, Loss: 0.8859100937843323\n",
      "Epoch 5, Loss: 0.7551162242889404\n",
      "Epoch 5, Loss: 0.8074009418487549\n",
      "Epoch 5, Loss: 0.6457540988922119\n",
      "Epoch 5, Loss: 0.8559409976005554\n",
      "Epoch 5, Loss: 0.3091537058353424\n",
      "Epoch 5, Loss: 0.8917577862739563\n",
      "Epoch 6, Loss: 0.6862229704856873\n",
      "Epoch 6, Loss: 0.4274959862232208\n",
      "Epoch 6, Loss: 0.40669283270835876\n",
      "Epoch 6, Loss: 0.45872631669044495\n",
      "Epoch 6, Loss: 0.3031126856803894\n",
      "Epoch 6, Loss: 0.39093226194381714\n",
      "Epoch 6, Loss: 0.45285674929618835\n",
      "Epoch 6, Loss: 0.7026243209838867\n",
      "Epoch 6, Loss: 0.41153115034103394\n",
      "Epoch 6, Loss: 0.32829782366752625\n",
      "Epoch 6, Loss: 0.4083164930343628\n",
      "Epoch 6, Loss: 0.6978960037231445\n",
      "Epoch 6, Loss: 0.6540178060531616\n",
      "Epoch 6, Loss: 0.4647057354450226\n",
      "Epoch 6, Loss: 0.5490501523017883\n",
      "Epoch 6, Loss: 0.5856264233589172\n",
      "Epoch 6, Loss: 0.42139777541160583\n",
      "Epoch 6, Loss: 0.5164943337440491\n",
      "Epoch 6, Loss: 0.2836371660232544\n",
      "Epoch 6, Loss: 0.48544108867645264\n",
      "Epoch 6, Loss: 0.3333834409713745\n",
      "Epoch 6, Loss: 0.42864710092544556\n",
      "Epoch 6, Loss: 0.49215444922447205\n",
      "Epoch 6, Loss: 0.32870349287986755\n",
      "Epoch 6, Loss: 0.8631711602210999\n",
      "Epoch 6, Loss: 0.4177830219268799\n",
      "Epoch 6, Loss: 0.3119055926799774\n",
      "Epoch 6, Loss: 0.3137296438217163\n",
      "Epoch 6, Loss: 0.38533613085746765\n",
      "Epoch 6, Loss: 0.5504991412162781\n",
      "Epoch 6, Loss: 0.3571542501449585\n",
      "Epoch 6, Loss: 0.2210875153541565\n",
      "Epoch 6, Loss: 0.604460597038269\n",
      "Epoch 6, Loss: 0.5228238701820374\n",
      "Epoch 6, Loss: 0.3572620749473572\n",
      "Epoch 6, Loss: 0.46616870164871216\n",
      "Epoch 6, Loss: 0.5090618133544922\n",
      "Epoch 6, Loss: 0.3958542048931122\n",
      "Epoch 6, Loss: 0.3539312481880188\n",
      "Epoch 6, Loss: 0.5126715898513794\n",
      "Epoch 6, Loss: 0.5855646133422852\n",
      "Epoch 6, Loss: 0.35155603289604187\n",
      "Epoch 6, Loss: 0.2113240659236908\n",
      "Epoch 6, Loss: 0.38740354776382446\n",
      "Epoch 6, Loss: 0.41554585099220276\n",
      "Epoch 6, Loss: 0.7170078754425049\n",
      "Epoch 6, Loss: 0.23351621627807617\n",
      "Epoch 6, Loss: 0.5525108575820923\n",
      "Epoch 6, Loss: 0.3692740797996521\n",
      "Epoch 6, Loss: 0.3606020510196686\n",
      "Epoch 6, Loss: 0.24005989730358124\n",
      "Epoch 6, Loss: 0.5203316807746887\n",
      "Epoch 6, Loss: 0.4369378685951233\n",
      "Epoch 6, Loss: 0.9624426364898682\n",
      "Epoch 6, Loss: 0.3154405355453491\n",
      "Epoch 6, Loss: 0.6145733594894409\n",
      "Epoch 6, Loss: 0.567269504070282\n",
      "Epoch 6, Loss: 0.5615832209587097\n",
      "Epoch 6, Loss: 0.45532310009002686\n",
      "Epoch 6, Loss: 0.409992516040802\n",
      "Epoch 6, Loss: 0.3394629955291748\n",
      "Epoch 6, Loss: 0.4021136462688446\n",
      "Epoch 6, Loss: 0.3687595725059509\n",
      "Epoch 6, Loss: 0.40429434180259705\n",
      "Epoch 6, Loss: 0.4485650956630707\n",
      "Epoch 6, Loss: 0.5481832027435303\n",
      "Epoch 6, Loss: 0.5191864371299744\n",
      "Epoch 6, Loss: 0.4052469730377197\n",
      "Epoch 6, Loss: 0.711524486541748\n",
      "Epoch 6, Loss: 0.42752939462661743\n",
      "Epoch 6, Loss: 0.5228923559188843\n",
      "Epoch 6, Loss: 0.2528282403945923\n",
      "Epoch 6, Loss: 0.4523094594478607\n",
      "Epoch 6, Loss: 0.43869876861572266\n",
      "Epoch 6, Loss: 0.9593603610992432\n",
      "Epoch 6, Loss: 0.49386030435562134\n",
      "Epoch 6, Loss: 0.5001278519630432\n",
      "Epoch 6, Loss: 0.33508145809173584\n",
      "Epoch 6, Loss: 0.3623179495334625\n",
      "Epoch 7, Loss: 0.3427019715309143\n",
      "Epoch 7, Loss: 0.21179012954235077\n",
      "Epoch 7, Loss: 0.17443472146987915\n",
      "Epoch 7, Loss: 0.2751489281654358\n",
      "Epoch 7, Loss: 0.34896987676620483\n",
      "Epoch 7, Loss: 0.20937082171440125\n",
      "Epoch 7, Loss: 0.2106856405735016\n",
      "Epoch 7, Loss: 0.3886799216270447\n",
      "Epoch 7, Loss: 0.2591150403022766\n",
      "Epoch 7, Loss: 0.2920202612876892\n",
      "Epoch 7, Loss: 0.10533887147903442\n",
      "Epoch 7, Loss: 0.22634468972682953\n",
      "Epoch 7, Loss: 0.20339879393577576\n",
      "Epoch 7, Loss: 0.23817376792430878\n",
      "Epoch 7, Loss: 0.1754325032234192\n",
      "Epoch 7, Loss: 0.24389085173606873\n",
      "Epoch 7, Loss: 0.25643160939216614\n",
      "Epoch 7, Loss: 0.19113770127296448\n",
      "Epoch 7, Loss: 0.19572149217128754\n",
      "Epoch 7, Loss: 0.183260977268219\n",
      "Epoch 7, Loss: 0.3560523986816406\n",
      "Epoch 7, Loss: 0.1836349219083786\n",
      "Epoch 7, Loss: 0.443261057138443\n",
      "Epoch 7, Loss: 0.13619141280651093\n",
      "Epoch 7, Loss: 0.20552222430706024\n",
      "Epoch 7, Loss: 0.24213263392448425\n",
      "Epoch 7, Loss: 0.2955247163772583\n",
      "Epoch 7, Loss: 0.223383828997612\n",
      "Epoch 7, Loss: 0.18651407957077026\n",
      "Epoch 7, Loss: 0.2669772207736969\n",
      "Epoch 7, Loss: 0.14416712522506714\n",
      "Epoch 7, Loss: 0.11993678659200668\n",
      "Epoch 7, Loss: 0.28662872314453125\n",
      "Epoch 7, Loss: 0.10307788848876953\n",
      "Epoch 7, Loss: 0.17801304161548615\n",
      "Epoch 7, Loss: 0.3282659649848938\n",
      "Epoch 7, Loss: 0.17007023096084595\n",
      "Epoch 7, Loss: 0.2118091732263565\n",
      "Epoch 7, Loss: 0.23112446069717407\n",
      "Epoch 7, Loss: 0.31830400228500366\n",
      "Epoch 7, Loss: 0.2997940480709076\n",
      "Epoch 7, Loss: 0.42646852135658264\n",
      "Epoch 7, Loss: 0.18753698468208313\n",
      "Epoch 7, Loss: 0.20230939984321594\n",
      "Epoch 7, Loss: 0.20414187014102936\n",
      "Epoch 7, Loss: 0.3472960591316223\n",
      "Epoch 7, Loss: 0.3180725872516632\n",
      "Epoch 7, Loss: 0.13230980932712555\n",
      "Epoch 7, Loss: 0.25462606549263\n",
      "Epoch 7, Loss: 0.17591525614261627\n",
      "Epoch 7, Loss: 0.1774839162826538\n",
      "Epoch 7, Loss: 0.22853857278823853\n",
      "Epoch 7, Loss: 0.16464659571647644\n",
      "Epoch 7, Loss: 0.4547387361526489\n",
      "Epoch 7, Loss: 0.13024428486824036\n",
      "Epoch 7, Loss: 0.33346641063690186\n",
      "Epoch 7, Loss: 0.3157631456851959\n",
      "Epoch 7, Loss: 0.25424084067344666\n",
      "Epoch 7, Loss: 0.39046794176101685\n",
      "Epoch 7, Loss: 0.24069838225841522\n",
      "Epoch 7, Loss: 0.44228702783584595\n",
      "Epoch 7, Loss: 0.3822682797908783\n",
      "Epoch 7, Loss: 0.45872074365615845\n",
      "Epoch 7, Loss: 0.4067818224430084\n",
      "Epoch 7, Loss: 0.19248458743095398\n",
      "Epoch 7, Loss: 0.14612630009651184\n",
      "Epoch 7, Loss: 0.30935800075531006\n",
      "Epoch 7, Loss: 0.30645427107810974\n",
      "Epoch 7, Loss: 0.32255685329437256\n",
      "Epoch 7, Loss: 0.29166528582572937\n",
      "Epoch 7, Loss: 0.21671220660209656\n",
      "Epoch 7, Loss: 0.25261449813842773\n",
      "Epoch 7, Loss: 0.30529433488845825\n",
      "Epoch 7, Loss: 0.24420076608657837\n",
      "Epoch 7, Loss: 0.3518860340118408\n",
      "Epoch 7, Loss: 0.3495955765247345\n",
      "Epoch 7, Loss: 0.29691407084465027\n",
      "Epoch 7, Loss: 0.1727362871170044\n",
      "Epoch 7, Loss: 0.3783099949359894\n",
      "Epoch 8, Loss: 0.20410647988319397\n",
      "Epoch 8, Loss: 0.09618348628282547\n",
      "Epoch 8, Loss: 0.3023008108139038\n",
      "Epoch 8, Loss: 0.2051381915807724\n",
      "Epoch 8, Loss: 0.23976701498031616\n",
      "Epoch 8, Loss: 0.202823206782341\n",
      "Epoch 8, Loss: 0.37609046697616577\n",
      "Epoch 8, Loss: 0.30139607191085815\n",
      "Epoch 8, Loss: 0.1120266243815422\n",
      "Epoch 8, Loss: 0.09076599776744843\n",
      "Epoch 8, Loss: 0.1249149963259697\n",
      "Epoch 8, Loss: 0.18909786641597748\n",
      "Epoch 8, Loss: 0.12398698180913925\n",
      "Epoch 8, Loss: 0.20289665460586548\n",
      "Epoch 8, Loss: 0.10172364860773087\n",
      "Epoch 8, Loss: 0.2265082150697708\n",
      "Epoch 8, Loss: 0.09268942475318909\n",
      "Epoch 8, Loss: 0.061646852642297745\n",
      "Epoch 8, Loss: 0.09052427858114243\n",
      "Epoch 8, Loss: 0.14520440995693207\n",
      "Epoch 8, Loss: 0.1489223688840866\n",
      "Epoch 8, Loss: 0.08484983444213867\n",
      "Epoch 8, Loss: 0.17475314438343048\n",
      "Epoch 8, Loss: 0.09996362030506134\n",
      "Epoch 8, Loss: 0.020500756800174713\n",
      "Epoch 8, Loss: 0.05155584216117859\n",
      "Epoch 8, Loss: 0.04244355857372284\n",
      "Epoch 8, Loss: 0.12705320119857788\n",
      "Epoch 8, Loss: 0.11962871998548508\n",
      "Epoch 8, Loss: 0.08637963980436325\n",
      "Epoch 8, Loss: 0.13579091429710388\n",
      "Epoch 8, Loss: 0.1963566392660141\n",
      "Epoch 8, Loss: 0.09922322630882263\n",
      "Epoch 8, Loss: 0.06827583909034729\n",
      "Epoch 8, Loss: 0.14108867943286896\n",
      "Epoch 8, Loss: 0.12006113678216934\n",
      "Epoch 8, Loss: 0.20018364489078522\n",
      "Epoch 8, Loss: 0.050576526671648026\n",
      "Epoch 8, Loss: 0.057823896408081055\n",
      "Epoch 8, Loss: 0.029295364394783974\n",
      "Epoch 8, Loss: 0.0656827911734581\n",
      "Epoch 8, Loss: 0.21288244426250458\n",
      "Epoch 8, Loss: 0.03989357128739357\n",
      "Epoch 8, Loss: 0.13959819078445435\n",
      "Epoch 8, Loss: 0.1850673258304596\n",
      "Epoch 8, Loss: 0.0630088523030281\n",
      "Epoch 8, Loss: 0.11345987021923065\n",
      "Epoch 8, Loss: 0.07675514370203018\n",
      "Epoch 8, Loss: 0.1834888756275177\n",
      "Epoch 8, Loss: 0.18540509045124054\n",
      "Epoch 8, Loss: 0.062456123530864716\n",
      "Epoch 8, Loss: 0.0963919460773468\n",
      "Epoch 8, Loss: 0.1498105376958847\n",
      "Epoch 8, Loss: 0.17457222938537598\n",
      "Epoch 8, Loss: 0.16983994841575623\n",
      "Epoch 8, Loss: 0.054849687963724136\n",
      "Epoch 8, Loss: 0.11793723702430725\n",
      "Epoch 8, Loss: 0.09345487505197525\n",
      "Epoch 8, Loss: 0.10525860637426376\n",
      "Epoch 8, Loss: 0.1419702023267746\n",
      "Epoch 8, Loss: 0.1475929468870163\n",
      "Epoch 8, Loss: 0.33568865060806274\n",
      "Epoch 8, Loss: 0.20592786371707916\n",
      "Epoch 8, Loss: 0.13954493403434753\n",
      "Epoch 8, Loss: 0.2640666365623474\n",
      "Epoch 8, Loss: 0.04166979715228081\n",
      "Epoch 8, Loss: 0.05443036928772926\n",
      "Epoch 8, Loss: 0.29344695806503296\n",
      "Epoch 8, Loss: 0.11154748499393463\n",
      "Epoch 8, Loss: 0.06731510162353516\n",
      "Epoch 8, Loss: 0.2550482451915741\n",
      "Epoch 8, Loss: 0.1837804913520813\n",
      "Epoch 8, Loss: 0.14215411245822906\n",
      "Epoch 8, Loss: 0.1260523945093155\n",
      "Epoch 8, Loss: 0.11127518862485886\n",
      "Epoch 8, Loss: 0.13009658455848694\n",
      "Epoch 8, Loss: 0.29419177770614624\n",
      "Epoch 8, Loss: 0.30462542176246643\n",
      "Epoch 8, Loss: 0.3678247928619385\n",
      "Epoch 9, Loss: 0.09410171955823898\n",
      "Epoch 9, Loss: 0.08128101378679276\n",
      "Epoch 9, Loss: 0.06065007299184799\n",
      "Epoch 9, Loss: 0.08016440272331238\n",
      "Epoch 9, Loss: 0.11532937735319138\n",
      "Epoch 9, Loss: 0.06583661586046219\n",
      "Epoch 9, Loss: 0.1867922693490982\n",
      "Epoch 9, Loss: 0.13453082740306854\n",
      "Epoch 9, Loss: 0.036317579448223114\n",
      "Epoch 9, Loss: 0.07348489761352539\n",
      "Epoch 9, Loss: 0.08044931292533875\n",
      "Epoch 9, Loss: 0.05392424762248993\n",
      "Epoch 9, Loss: 0.041855037212371826\n",
      "Epoch 9, Loss: 0.06534097343683243\n",
      "Epoch 9, Loss: 0.09404739737510681\n",
      "Epoch 9, Loss: 0.06979995965957642\n",
      "Epoch 9, Loss: 0.09750191122293472\n",
      "Epoch 9, Loss: 0.05939289554953575\n",
      "Epoch 9, Loss: 0.056596748530864716\n",
      "Epoch 9, Loss: 0.05810078978538513\n",
      "Epoch 9, Loss: 0.04175471141934395\n",
      "Epoch 9, Loss: 0.10202772170305252\n",
      "Epoch 9, Loss: 0.028603030368685722\n",
      "Epoch 9, Loss: 0.08752475678920746\n",
      "Epoch 9, Loss: 0.04222261160612106\n",
      "Epoch 9, Loss: 0.17275378108024597\n",
      "Epoch 9, Loss: 0.05229645222425461\n",
      "Epoch 9, Loss: 0.10813228785991669\n",
      "Epoch 9, Loss: 0.054037969559431076\n",
      "Epoch 9, Loss: 0.05590559542179108\n",
      "Epoch 9, Loss: 0.0968247503042221\n",
      "Epoch 9, Loss: 0.0994253158569336\n",
      "Epoch 9, Loss: 0.04200856387615204\n",
      "Epoch 9, Loss: 0.0340845100581646\n",
      "Epoch 9, Loss: 0.06893935799598694\n",
      "Epoch 9, Loss: 0.07448603212833405\n",
      "Epoch 9, Loss: 0.03342517465353012\n",
      "Epoch 9, Loss: 0.08351699262857437\n",
      "Epoch 9, Loss: 0.0675387904047966\n",
      "Epoch 9, Loss: 0.01358270738273859\n",
      "Epoch 9, Loss: 0.09433943778276443\n",
      "Epoch 9, Loss: 0.1163671612739563\n",
      "Epoch 9, Loss: 0.034777555614709854\n",
      "Epoch 9, Loss: 0.05075403302907944\n",
      "Epoch 9, Loss: 0.09330421686172485\n",
      "Epoch 9, Loss: 0.03817668929696083\n",
      "Epoch 9, Loss: 0.050465069711208344\n",
      "Epoch 9, Loss: 0.034767791628837585\n",
      "Epoch 9, Loss: 0.02483374997973442\n",
      "Epoch 9, Loss: 0.04985975846648216\n",
      "Epoch 9, Loss: 0.02762133814394474\n",
      "Epoch 9, Loss: 0.044051602482795715\n",
      "Epoch 9, Loss: 0.04998160898685455\n",
      "Epoch 9, Loss: 0.03612542524933815\n",
      "Epoch 9, Loss: 0.10421822220087051\n",
      "Epoch 9, Loss: 0.13658523559570312\n",
      "Epoch 9, Loss: 0.12943528592586517\n",
      "Epoch 9, Loss: 0.03380395472049713\n",
      "Epoch 9, Loss: 0.1262747347354889\n",
      "Epoch 9, Loss: 0.05848131328821182\n",
      "Epoch 9, Loss: 0.0710013285279274\n",
      "Epoch 9, Loss: 0.08874556422233582\n",
      "Epoch 9, Loss: 0.0857173502445221\n",
      "Epoch 9, Loss: 0.03530149906873703\n",
      "Epoch 9, Loss: 0.09109419584274292\n",
      "Epoch 9, Loss: 0.040672171860933304\n",
      "Epoch 9, Loss: 0.06021267548203468\n",
      "Epoch 9, Loss: 0.046672090888023376\n",
      "Epoch 9, Loss: 0.03442448377609253\n",
      "Epoch 9, Loss: 0.062444042414426804\n",
      "Epoch 9, Loss: 0.14766089618206024\n",
      "Epoch 9, Loss: 0.1287984400987625\n",
      "Epoch 9, Loss: 0.10376301407814026\n",
      "Epoch 9, Loss: 0.03718113154172897\n",
      "Epoch 9, Loss: 0.013482784852385521\n",
      "Epoch 9, Loss: 0.06325112283229828\n",
      "Epoch 9, Loss: 0.06226892024278641\n",
      "Epoch 9, Loss: 0.07364337146282196\n",
      "Epoch 9, Loss: 0.16911716759204865\n",
      "Epoch 10, Loss: 0.06555812060832977\n",
      "Epoch 10, Loss: 0.05761415883898735\n",
      "Epoch 10, Loss: 0.015468880534172058\n",
      "Epoch 10, Loss: 0.008025223389267921\n",
      "Epoch 10, Loss: 0.027075015008449554\n",
      "Epoch 10, Loss: 0.013733360916376114\n",
      "Epoch 10, Loss: 0.09158522635698318\n",
      "Epoch 10, Loss: 0.10126739740371704\n",
      "Epoch 10, Loss: 0.03159753978252411\n",
      "Epoch 10, Loss: 0.07731405645608902\n",
      "Epoch 10, Loss: 0.06002407893538475\n",
      "Epoch 10, Loss: 0.049698684364557266\n",
      "Epoch 10, Loss: 0.01075745839625597\n",
      "Epoch 10, Loss: 0.009915554895997047\n",
      "Epoch 10, Loss: 0.0272245854139328\n",
      "Epoch 10, Loss: 0.0336940735578537\n",
      "Epoch 10, Loss: 0.07185471802949905\n",
      "Epoch 10, Loss: 0.06844697147607803\n",
      "Epoch 10, Loss: 0.02251022681593895\n",
      "Epoch 10, Loss: 0.021976713091135025\n",
      "Epoch 10, Loss: 0.042135436087846756\n",
      "Epoch 10, Loss: 0.01078025158494711\n",
      "Epoch 10, Loss: 0.032416243106126785\n",
      "Epoch 10, Loss: 0.015834975987672806\n",
      "Epoch 10, Loss: 0.02053219825029373\n",
      "Epoch 10, Loss: 0.040259476751089096\n",
      "Epoch 10, Loss: 0.011502381414175034\n",
      "Epoch 10, Loss: 0.026471365243196487\n",
      "Epoch 10, Loss: 0.025908518582582474\n",
      "Epoch 10, Loss: 0.03836604207754135\n",
      "Epoch 10, Loss: 0.01849682256579399\n",
      "Epoch 10, Loss: 0.015917649492621422\n",
      "Epoch 10, Loss: 0.00917296763509512\n",
      "Epoch 10, Loss: 0.02885536663234234\n",
      "Epoch 10, Loss: 0.01748422160744667\n",
      "Epoch 10, Loss: 0.03500407561659813\n",
      "Epoch 10, Loss: 0.028769809752702713\n",
      "Epoch 10, Loss: 0.05449342355132103\n",
      "Epoch 10, Loss: 0.03157021850347519\n",
      "Epoch 10, Loss: 0.04188411310315132\n",
      "Epoch 10, Loss: 0.017748435959219933\n",
      "Epoch 10, Loss: 0.019955676048994064\n",
      "Epoch 10, Loss: 0.014467094093561172\n",
      "Epoch 10, Loss: 0.01822040230035782\n",
      "Epoch 10, Loss: 0.007131444755941629\n",
      "Epoch 10, Loss: 0.03167232871055603\n",
      "Epoch 10, Loss: 0.012545204721391201\n",
      "Epoch 10, Loss: 0.04056098312139511\n",
      "Epoch 10, Loss: 0.012683834880590439\n",
      "Epoch 10, Loss: 0.041665881872177124\n",
      "Epoch 10, Loss: 0.0335942842066288\n",
      "Epoch 10, Loss: 0.05112336203455925\n",
      "Epoch 10, Loss: 0.08413520455360413\n",
      "Epoch 10, Loss: 0.010120082646608353\n",
      "Epoch 10, Loss: 0.013725482858717442\n",
      "Epoch 10, Loss: 0.012142430990934372\n",
      "Epoch 10, Loss: 0.019252372905611992\n",
      "Epoch 10, Loss: 0.013709692284464836\n",
      "Epoch 10, Loss: 0.022099949419498444\n",
      "Epoch 10, Loss: 0.00543578527867794\n",
      "Epoch 10, Loss: 0.006833259016275406\n",
      "Epoch 10, Loss: 0.10721387714147568\n",
      "Epoch 10, Loss: 0.013680640608072281\n",
      "Epoch 10, Loss: 0.014938618056476116\n",
      "Epoch 10, Loss: 0.10620502382516861\n",
      "Epoch 10, Loss: 0.02105564810335636\n",
      "Epoch 10, Loss: 0.03110266663134098\n",
      "Epoch 10, Loss: 0.03464554622769356\n",
      "Epoch 10, Loss: 0.18336759507656097\n",
      "Epoch 10, Loss: 0.04980810731649399\n",
      "Epoch 10, Loss: 0.07741806656122208\n",
      "Epoch 10, Loss: 0.03994120657444\n",
      "Epoch 10, Loss: 0.012554901652038097\n",
      "Epoch 10, Loss: 0.06178433820605278\n",
      "Epoch 10, Loss: 0.018040532246232033\n",
      "Epoch 10, Loss: 0.009588935412466526\n",
      "Epoch 10, Loss: 0.042101506143808365\n",
      "Epoch 10, Loss: 0.01365373283624649\n",
      "Epoch 10, Loss: 0.02503051422536373\n",
      "Accuracy of the network on fold 1 test images: 54.04761904761905%\n",
      "Training on Fold 2\n",
      "Epoch 1, Loss: 1.1599258184432983\n",
      "Epoch 1, Loss: 1.2150232791900635\n",
      "Epoch 1, Loss: 3.9441657066345215\n",
      "Epoch 1, Loss: 1.6012351512908936\n",
      "Epoch 1, Loss: 1.4820098876953125\n",
      "Epoch 1, Loss: 2.3174173831939697\n",
      "Epoch 1, Loss: 0.842261791229248\n",
      "Epoch 1, Loss: 1.880258560180664\n",
      "Epoch 1, Loss: 1.2082669734954834\n",
      "Epoch 1, Loss: 0.9777033925056458\n",
      "Epoch 1, Loss: 0.8812661170959473\n",
      "Epoch 1, Loss: 1.2530440092086792\n",
      "Epoch 1, Loss: 1.4641485214233398\n",
      "Epoch 1, Loss: 0.9985843896865845\n",
      "Epoch 1, Loss: 1.5936769247055054\n",
      "Epoch 1, Loss: 0.9881278276443481\n",
      "Epoch 1, Loss: 1.001895546913147\n",
      "Epoch 1, Loss: 0.922208845615387\n",
      "Epoch 1, Loss: 0.807329535484314\n",
      "Epoch 1, Loss: 0.739641010761261\n",
      "Epoch 1, Loss: 1.361093282699585\n",
      "Epoch 1, Loss: 1.1133348941802979\n",
      "Epoch 1, Loss: 0.8814108967781067\n",
      "Epoch 1, Loss: 1.226765513420105\n",
      "Epoch 1, Loss: 1.174717664718628\n",
      "Epoch 1, Loss: 1.131683588027954\n",
      "Epoch 1, Loss: 0.9879546761512756\n",
      "Epoch 1, Loss: 0.8050055503845215\n",
      "Epoch 1, Loss: 1.707330346107483\n",
      "Epoch 1, Loss: 1.0016095638275146\n",
      "Epoch 1, Loss: 0.6544820070266724\n",
      "Epoch 1, Loss: 0.8697494864463806\n",
      "Epoch 1, Loss: 1.114694595336914\n",
      "Epoch 1, Loss: 1.0143071413040161\n",
      "Epoch 1, Loss: 1.019566535949707\n",
      "Epoch 1, Loss: 0.9445360898971558\n",
      "Epoch 1, Loss: 0.6797749400138855\n",
      "Epoch 1, Loss: 0.4724707007408142\n",
      "Epoch 1, Loss: 1.2604955434799194\n",
      "Epoch 1, Loss: 0.8094016909599304\n",
      "Epoch 1, Loss: 1.0337951183319092\n",
      "Epoch 1, Loss: 0.962939441204071\n",
      "Epoch 1, Loss: 1.055868148803711\n",
      "Epoch 1, Loss: 0.7047458291053772\n",
      "Epoch 1, Loss: 1.523748755455017\n",
      "Epoch 1, Loss: 1.113048791885376\n",
      "Epoch 1, Loss: 0.7111905813217163\n",
      "Epoch 1, Loss: 0.9512777328491211\n",
      "Epoch 1, Loss: 0.923312246799469\n",
      "Epoch 1, Loss: 1.1360878944396973\n",
      "Epoch 1, Loss: 0.7748149633407593\n",
      "Epoch 1, Loss: 0.815479040145874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [109], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pratham Sharda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "data_dir = 'D:/SRIP/5_class_dataset'\n",
    "\n",
    "for fold in range(1, 4):\n",
    "    print(f\"Training on Fold {fold}\")\n",
    "    train_dataset, test_dataset = load_datasets(data_dir, fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "            \n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "    print(f'Accuracy of the network on fold {fold} test images: {100 * correct / total}%')\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f'Confusion Matrix for Fold {fold}')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f'Classification Report for Fold {fold}:')\n",
    "    print(classification_report(true_labels, predicted_labels, target_names=train_dataset.classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
